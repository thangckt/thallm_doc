{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>thml</code> Documentation","text":""},{"location":"#thml","title":"<code>thml</code>","text":"<p>The package for LLM applications</p> <p>Developed and maintained by C.Thang Nguyen</p>"},{"location":"llm_chat/","title":"llm_chat","text":""},{"location":"llm_chat/#thml.llm_chat","title":"<code>thml.llm_chat</code>","text":"<p>This module contains the chatbot classes and methods to work with text generation models.</p> <p>Modules:</p> <ul> <li> <code>post_api</code>           \u2013            <p>This module contains the chatbot classes and methods to work with text generation models.</p> </li> <li> <code>web_playwright</code>           \u2013            </li> <li> <code>web_requests</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api","title":"<code>post_api</code>","text":"<p>This module contains the chatbot classes and methods to work with text generation models.</p> <p>Modules:</p> <ul> <li> <code>chat_google</code>           \u2013            <p>Build a chatbot using Google's Gemini API</p> </li> <li> <code>chat_gpt4free</code>           \u2013            </li> <li> <code>chat_openai</code>           \u2013            <p>Using chatGPT API to build a chatbot</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google","title":"<code>chat_google</code>","text":"<p>Build a chatbot using Google's Gemini API</p> <p>REF: - Python API: https://ai.google.dev/tutorials/python_quickstart - Prompt examples: https://ai.google.dev/docs/prompt_best_practices - Palm vs Gemini: https://ai.google.dev/docs/migration_guide</p> <p>Classes:</p> <ul> <li> <code>Google</code>           \u2013            <p>Class for chatbot using Google's Gemini API via google.generativeai package</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google","title":"<code>Google(api_key=None, **kwargs)</code>","text":"<p>Class for chatbot using Google's Gemini API via google.generativeai package</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The OpenAI API key.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>model</code>               (<code>str = 'gemini-pro'</code>)           \u2013            <p>The model to use for the chat client.</p> </li> <li> <code>temperature</code>               (<code>float = 0.7</code>)           \u2013            <p>The temperature to use for the chat client.</p> </li> <li> <code>top_p</code>               (<code>float = 1</code>)           \u2013            <p>An alternative to sampling with temperature.</p> </li> <li> <code>max_tokens</code>               (<code>int = 8096</code>)           \u2013            <p>The maximum number of tokens to generate in the response.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask Google Gemini a question and return the answer.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>avail_models</code>           \u2013            </li> <li> <code>params</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google.params","title":"<code>params = kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google.ask","title":"<code>ask(prompt='hello my friend')</code>","text":"<p>Ask Google Gemini a question and return the answer.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'hello my friend'</code> )           \u2013            <p>The question or prompt to ask the chatbot.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>text</code> (              <code>str</code> )          \u2013            <p>The answer to the question.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free","title":"<code>chat_gpt4free</code>","text":"<p>Classes:</p> <ul> <li> <code>FreeChat</code>           \u2013            <p>Class for chatbot using reverse enginerring models.</p> </li> <li> <code>FreeImage</code>           \u2013            <p>Class for image generation using reverse-enginered models.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat","title":"<code>FreeChat(**kwargs: Any)</code>","text":"<p>               Bases: <code>_Base</code></p> <p>Class for chatbot using reverse enginerring models.</p> <p>Other Parameters:</p> <ul> <li> <code>provider</code>               (<code>str = None</code>)           \u2013            <p>The provider of the model. If None, the best provider will be used.</p> </li> <li> <code>api_key</code>               (<code>str = None</code>)           \u2013            <p>The API key for the provider.</p> </li> <li> <code>model</code>               (<code>str = 'gpt-4'</code>)           \u2013            <p>The model to use.</p> </li> <li> <code>temperature</code>               (<code>float = 0.7</code>)           \u2013            <p>The temperature of the model.</p> </li> <li> <code>top_p</code>               (<code>float = 1</code>)           \u2013            <p>The top_p of the model.</p> </li> <li> <code>max_tokens</code>               (<code>int = 8096</code>)           \u2013            <p>The max tokens of the model.</p> </li> <li> <code>system_prompt</code>               (<code>str = ''</code>)           \u2013            <p>The system prompt of the model.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask the chatbot a question.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>params</code>           \u2013            </li> <li> <code>avail_models</code>           \u2013            </li> <li> <code>avail_providers</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.params","title":"<code>params = _params(**kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.avail_providers","title":"<code>avail_providers = self._avail_providers()</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.ask","title":"<code>ask(prompt: str) -&gt; str</code>","text":"<p>Ask the chatbot a question. Args:     prompt (str): The input string for the chatbot.</p> <p>Returns:</p> <ul> <li> <code>text</code> (              <code>str</code> )          \u2013            <p>The answer to the question.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeImage","title":"<code>FreeImage()</code>","text":"<p>Class for image generation using reverse-enginered models.</p>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai","title":"<code>chat_openai</code>","text":"<p>Using chatGPT API to build a chatbot</p> <p>Implementation following this repo: https://github.com/stancsz/chatgpt/blob/master/ChatGPT.py</p> <p>Terms: - OpenAI's text generation models (often called generative pre-trained transformers or large language models) - The inputs to these models are also referred to as \"prompts\".</p> <p>REF: - Refer to file: src_thatool\\devtools\\dev_chatGPT\\chat_API/chat_Copilot2GPT.ipynb - openai docs: https://platform.openai.com/docs/guides/text-generation/text-generation-models - openai repo: https://github.com/openai/openai-python - prompt examples: https://github.com/f/awesome-chatgpt-prompts - openai examples: https://platform.openai.com/examples - inherent class in python: https://stackoverflow.com/questions/9575409/calling-parent-class-init-with-multiple-inheritance-whats-the-right-way - super() in python: https://stackoverflow.com/questions/34550425/how-to-initialize-subclass-paramseters-in-python-using-super - create chatbot using openai API: https://medium.com/data-professor/beginners-guide-to-openai-api-a0420bc58ee5 - OpenAI API tips: https://arize.com/blog-course/mastering-openai-api-tips-and-tricks/</p> <p>Classes:</p> <ul> <li> <code>BaseChat</code>           \u2013            <p>Base class for chatbot, to define common attributes and methods for chatbot</p> </li> <li> <code>Openai</code>           \u2013            <p>Class for chatbot using OpenAI API via <code>openai</code> package</p> </li> <li> <code>Post</code>           \u2013            <p>Class for chatbot using OpenAI API via POST request</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat","title":"<code>BaseChat</code>","text":"<p>Base class for chatbot, to define common attributes and methods for chatbot</p> <p>Methods:</p> <ul> <li> <code>save_history</code>             \u2013              </li> <li> <code>export_history</code>             \u2013              </li> <li> <code>load_history</code>             \u2013              </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat.save_history","title":"<code>save_history(prompt, response)</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat.export_history","title":"<code>export_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat.load_history","title":"<code>load_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai","title":"<code>Openai(service: str = 'openai', **kwargs: Any)</code>","text":"<p>               Bases: <code>BaseChat</code></p> <p>Class for chatbot using OpenAI API via <code>openai</code> package</p> <p>Parameters:</p> <ul> <li> <code>service</code>               (<code>str</code>, default:                   <code>'openai'</code> )           \u2013            <p>The service to use for the chat client (preset of base_url). Available services are: openai, copilot, local_gpt4all</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>base_url</code>               (<code>str</code>)           \u2013            <p>The OpenAI API base URL. Presetted based on the service.</p> </li> <li> <code>api_key</code>               (<code>str</code>)           \u2013            <p>The OpenAI API key. Presetted based on the service.</p> </li> <li> <code>model</code>               (<code>str = 'gpt-4'</code>)           \u2013            <p>The model to use for the chat client. All models can be found at the OpenAI site. Only 2 models 'gpt-4' and 'gpt-3.5-turbo' for copilot.</p> </li> <li> <code>temperature</code>               (<code>float = 0.7</code>)           \u2013            <p>The temperature to use for the chat client. The temperature is a value between 0 and 1. Lower temperatures will cause the model to repeat itself more often, while higher temperatures will increase the model's diversity of responses. Use either <code>temperature</code> or <code>top_p</code>, but not both.</p> </li> <li> <code>top_p</code>               (<code>float = 1</code>)           \u2013            <p>An alternative to sampling with temperature. The top_p is a value between 0 and 1. Use either <code>temperature</code> or <code>top_p</code>, but not both.</p> </li> <li> <code>max_tokens</code>               (<code>int = 8096</code>)           \u2013            <p>The maximum number of tokens to generate in the response.</p> </li> <li> <code>stream</code>               (<code>bool = False</code>)           \u2013            <p>Whether to stream the response or not.</p> </li> <li> <code>system_prompt</code>               (<code>str = ''</code>)           \u2013            <p>The prompt to use for the system.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>save_history</code>             \u2013              </li> <li> <code>export_history</code>             \u2013              </li> <li> <code>load_history</code>             \u2013              </li> <li> <code>ask</code>             \u2013              <p>Ask GPT-4 a question and return the answer. Use new openai API</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>params</code>           \u2013            </li> <li> <code>avail_models</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.params","title":"<code>params = _params(service, **kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.save_history","title":"<code>save_history(prompt, response)</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.export_history","title":"<code>export_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.load_history","title":"<code>load_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.ask","title":"<code>ask(prompt='hello', **kwargs: Any) -&gt; str</code>","text":"<p>Ask GPT-4 a question and return the answer. Use new openai API</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'hello'</code> )           \u2013            <p>The question to ask GPT-4.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>save_history</code>               (<code>bool = False</code>)           \u2013            <p>Whether to save the question and answer to the chat history.</p> </li> <li> <code>use_history</code>               (<code>bool = False</code>)           \u2013            <p>Whether to use the chat history in the current request.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>text</code> (              <code>str</code> )          \u2013            <p>The answer to the question.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post","title":"<code>Post(service: str = 'copilot', **kwargs: Any)</code>","text":"<p>Class for chatbot using OpenAI API via POST request</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>See <code>Openai</code> class for the arguments.</p> </li> </ul> Refs <ul> <li>https://www.techasoft.com/post/how-to-use-chatgpt-api-in-python-for-your-real-time-data)</li> <li>Curl vs python's requests: https://stackoverflow.com/questions/31061227/curl-vs-python-requests-when-hitting-apis</li> <li>Python and REST APIs: Interacting With Web Services: https://realpython.com/api-integration-in-python/</li> <li>Asynchronous Requests in Python: https://superfastpython.com/python-async-requests/</li> <li>Make request python faster: https://skillshats.com/blogs/optimize-python-requests-for-faster-performance/</li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>params</code>           \u2013            </li> <li> <code>avail_models</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post.params","title":"<code>params = _params(service, **kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post.ask","title":"<code>ask(prompt='hello')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright","title":"<code>web_playwright</code>","text":"<p>Modules:</p> <ul> <li> <code>chatgpt</code>           \u2013            </li> <li> <code>claude</code>           \u2013            </li> <li> <code>copilot_playwright</code>           \u2013            </li> <li> <code>gemini</code>           \u2013            </li> <li> <code>llama</code>           \u2013            </li> <li> <code>mistral</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt","title":"<code>chatgpt</code>","text":"<p>Classes:</p> <ul> <li> <code>WebChatgpt</code>           \u2013            <p>Interacte with chatgpt web</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt","title":"<code>WebChatgpt(cookie_file: str = None, proxy: str = None, chat_id: str = 'temporary')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'temporary'</code> )           \u2013            <p>\"id\", \"last\", \"temporary\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_page=\"temporary\"</code>. If \"last\", use last chat. If \"temporary\", use temporary chat (need to go wedsite to turn on this feature at the first time login of an account). If \"new\", start new chat.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask and get reponse from web</p> </li> <li> <code>get_chat_history</code>             \u2013              <p>alias of get_all_messages() method, but in sync mode</p> </li> <li> <code>close</code>             \u2013              <p>Alias of _close_page() method, but in sync mode</p> </li> <li> <code>send_prompt</code>             \u2013              <p>Submit prompt text</p> </li> <li> <code>get_last_ai_message</code>             \u2013              <p>Get the last AI message</p> </li> <li> <code>get_all_messages</code>             \u2013              <p>Get all messages by user and AI</p> </li> <li> <code>get_all_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> <li> <code>get_last_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>device</code>               (<code>str</code>)           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>send_count</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>cookie_file</code>           \u2013            </li> <li> <code>chat_id</code>           \u2013            </li> <li> <code>login</code>           \u2013            </li> <li> <code>prompt_textarea</code>           \u2013            </li> <li> <code>send_button</code>           \u2013            </li> <li> <code>stop_button</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.base_url","title":"<code>base_url = 'https://chatgpt.com'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.login","title":"<code>login = False</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_placeholder('Message ChatGPT')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.send_button","title":"<code>send_button = self.page.locator('button.mb-1.me-1.h-8.w-8')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.stop_button","title":"<code>stop_button = self.page.locator('button.mb-1.me-1.h-8.w-8').locator('rect')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_all_messages","title":"<code>get_all_messages() -&gt; list[dict]</code>  <code>async</code>","text":"<p>Get all messages by user and AI</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude","title":"<code>claude</code>","text":"<p>Classes:</p> <ul> <li> <code>WebClaude</code>           \u2013            <p>Interacte with chatgpt web</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude","title":"<code>WebClaude(cookie_file: str = None, proxy: str = None, chat_id: str = 'last')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'last'</code> )           \u2013            <p>\"id\", \"last\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_id=\"last\"</code>. If \"last\", use last_id. If \"new\", start new chat.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask and get reponse from web</p> </li> <li> <code>get_chat_history</code>             \u2013              <p>alias of get_all_messages() method, but in sync mode</p> </li> <li> <code>close</code>             \u2013              <p>Alias of _close_page() method, but in sync mode</p> </li> <li> <code>send_prompt</code>             \u2013              <p>Submit prompt text</p> </li> <li> <code>get_last_ai_message</code>             \u2013              <p>Get the last AI message</p> </li> <li> <code>get_all_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> <li> <code>get_last_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>device</code>               (<code>str</code>)           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>send_count</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>cookie_file</code>           \u2013            </li> <li> <code>chat_id</code>           \u2013            </li> <li> <code>prompt_textarea</code>           \u2013            </li> <li> <code>send_button</code>           \u2013            </li> <li> <code>stop_button</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.base_url","title":"<code>base_url = 'https://claude.ai'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_label('Write your prompt to Claude').locator('p')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Send Message')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop Response')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright","title":"<code>copilot_playwright</code>","text":"<p>Classes:</p> <ul> <li> <code>WebCopilot</code>           \u2013            <p>Interacte with chatgpt web</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot","title":"<code>WebCopilot(cookie_file: str = None, proxy: str = None, chat_id: str = None, converstion_style: str = None)</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask and get reponse from web</p> </li> <li> <code>get_chat_history</code>             \u2013              <p>alias of get_all_messages() method, but in sync mode</p> </li> <li> <code>close</code>             \u2013              <p>Alias of _close_page() method, but in sync mode</p> </li> <li> <code>send_prompt</code>             \u2013              <p>Submit prompt text</p> </li> <li> <code>get_last_ai_message</code>             \u2013              <p>Get the last AI message</p> </li> <li> <code>get_last_ai_reference</code>             \u2013              <p>Get the references in last AI message</p> </li> <li> <code>get_all_messages</code>             \u2013              <p>Get all messages by user and AI</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>device</code>               (<code>str</code>)           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>send_count</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>cookie_file</code>           \u2013            </li> <li> <code>prompt_textarea</code>           \u2013            </li> <li> <code>send_button</code>           \u2013            </li> <li> <code>stop_button</code>           \u2013            </li> <li> <code>upload_image_button</code>           \u2013            </li> <li> <code>upload_file_button</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.base_url","title":"<code>base_url = 'https://copilot.microsoft.com/'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_role('textbox', name='Ask me anything...')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Submit')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop Responding')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.upload_image_button","title":"<code>upload_image_button = self.page.get_by_role('button', name='Add an image to search')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.upload_file_button","title":"<code>upload_file_button = self.page.get_by_role('button', name='Add a file')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_last_ai_reference","title":"<code>get_last_ai_reference()</code>  <code>async</code>","text":"<p>Get the references in last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_all_messages","title":"<code>get_all_messages() -&gt; list[dict]</code>  <code>async</code>","text":"<p>Get all messages by user and AI</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini","title":"<code>gemini</code>","text":"<p>Classes:</p> <ul> <li> <code>WebGemini</code>           \u2013            <p>Interacte with chatgpt web</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini","title":"<code>WebGemini(cookie_file: str = None, proxy: str = None, conversation_id: str = None)</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask and get reponse from web</p> </li> <li> <code>get_chat_history</code>             \u2013              <p>alias of get_all_messages() method, but in sync mode</p> </li> <li> <code>close</code>             \u2013              <p>Alias of _close_page() method, but in sync mode</p> </li> <li> <code>send_prompt</code>             \u2013              <p>Submit prompt text</p> </li> <li> <code>get_last_message</code>             \u2013              <p>Get the last AI message</p> </li> <li> <code>get_all_messages</code>             \u2013              <p>Get all messages by user and AI</p> </li> <li> <code>up_load_file</code>             \u2013              <p>Get text</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>device</code>               (<code>str</code>)           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>send_count</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>cookie_file</code>           \u2013            </li> <li> <code>prompt_textarea</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.base_url","title":"<code>base_url = 'https://gemini.google.com/app'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_role('textbox')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.get_last_message","title":"<code>get_last_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.get_all_messages","title":"<code>get_all_messages() -&gt; list[dict]</code>  <code>async</code>","text":"<p>Get all messages by user and AI</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.up_load_file","title":"<code>up_load_file()</code>  <code>async</code>","text":"<p>Get text</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama","title":"<code>llama</code>","text":"<p>Classes:</p> <ul> <li> <code>WebLlama</code>           \u2013            <p>Interacte with chatgpt web</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama","title":"<code>WebLlama(cookie_file: str = None, proxy: str = None, chat_id: str = 'last')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'last'</code> )           \u2013            <p>\"id\", \"last\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_id=\"last\"</code>. If \"last\", use last_id. If \"new\", start new chat.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask and get reponse from web</p> </li> <li> <code>get_chat_history</code>             \u2013              <p>alias of get_all_messages() method, but in sync mode</p> </li> <li> <code>close</code>             \u2013              <p>Alias of _close_page() method, but in sync mode</p> </li> <li> <code>send_prompt</code>             \u2013              <p>Submit prompt text</p> </li> <li> <code>get_last_ai_message</code>             \u2013              <p>Get the last AI message</p> </li> <li> <code>get_all_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> <li> <code>get_last_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>device</code>               (<code>str</code>)           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>send_count</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>cookie_file</code>           \u2013            </li> <li> <code>chat_id</code>           \u2013            </li> <li> <code>prompt_textarea</code>           \u2013            </li> <li> <code>send_button</code>           \u2013            </li> <li> <code>stop_button</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.base_url","title":"<code>base_url = 'https://chatwithllama.com'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_placeholder('Ask anything!')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Send question')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop generation')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral","title":"<code>mistral</code>","text":"<p>Classes:</p> <ul> <li> <code>WebMistral</code>           \u2013            <p>Interacte with chatgpt web</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral","title":"<code>WebMistral(cookie_file: str = None, proxy: str = None, chat_id: str = 'last')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'last'</code> )           \u2013            <p>\"id\", \"last\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_id=\"last\"</code>. If \"last\", use last_id. If \"new\", start new chat.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask and get reponse from web</p> </li> <li> <code>get_chat_history</code>             \u2013              <p>alias of get_all_messages() method, but in sync mode</p> </li> <li> <code>close</code>             \u2013              <p>Alias of _close_page() method, but in sync mode</p> </li> <li> <code>send_prompt</code>             \u2013              <p>Submit prompt text</p> </li> <li> <code>get_last_ai_message</code>             \u2013              <p>Get the last AI message</p> </li> <li> <code>get_all_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> <li> <code>get_last_chat_id</code>             \u2013              <p>Get all conversation ids</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>device</code>               (<code>str</code>)           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>send_count</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>cookie_file</code>           \u2013            </li> <li> <code>chat_id</code>           \u2013            </li> <li> <code>prompt_textarea</code>           \u2013            </li> <li> <code>send_button</code>           \u2013            </li> <li> <code>stop_button</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.base_url","title":"<code>base_url = 'https://chat.mistral.ai'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_placeholder('Ask anything!')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Send question')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop generation')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_requests","title":"<code>web_requests</code>","text":"<p>Modules:</p> <ul> <li> <code>bing_copilot</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot","title":"<code>bing_copilot</code>","text":"<p>Classes:</p> <ul> <li> <code>RWebCopilot</code>           \u2013            <p>Reverse-engineered Bing/Edge Copilot via Web browser.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>response_parser</code>             \u2013              <p>Parse the response from the re_edge_gpt chatbot</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot","title":"<code>RWebCopilot(cookie_file: str = None, conversation_style: Literal['creative', 'balanced', 'precise'] = 'precise')</code>","text":"<p>Reverse-engineered Bing/Edge Copilot via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cookie file.</p> </li> <li> <code>conversation_style</code>               (<code>str</code>, default:                   <code>'precise'</code> )           \u2013            <p>The conversation style. Available options: 'creative', 'balanced', 'precise'</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Ask the bot a question</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>cookie_file</code>           \u2013            </li> <li> <code>bot</code>           \u2013            </li> <li> <code>conversation_style</code>           \u2013            </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.bot","title":"<code>bot = asyncio.run(Chatbot.create(cookies=cookies))</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.conversation_style","title":"<code>conversation_style = style_map[conversation_style]</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.ask","title":"<code>ask(prompt: str, attachment: str = None, return_refs: bool = False) -&gt; Union[str, dict[str, list]]</code>","text":"<p>Ask the bot a question</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>)           \u2013            <p>The prompt to ask the bot</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, dict[str, list]]</code>           \u2013            <p>dict[str, list[str]]: final_text, references</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.response_parser","title":"<code>response_parser(response: dict) -&gt; dict[str, list[str]]</code>","text":"<p>Parse the response from the re_edge_gpt chatbot</p> <p>Parameters:</p> <ul> <li> <code>response</code>               (<code>dict</code>)           \u2013            <p>response from the re_edge_gpt chatbot</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list[str]]</code>           \u2013            <p>dict[str, list[str]]: final_text, references</p> </li> </ul>"},{"location":"llm_langchain/","title":"llm_langchain","text":""},{"location":"llm_langchain/#thml.llm_langchain","title":"<code>thml.llm_langchain</code>","text":"<p>Interface LLM models into LangChain's style to be used in the RAG system.</p> <p>Classes:</p> <ul> <li> <code>G4FLLM</code>           \u2013            <p>Interface to the G4F service.</p> </li> <li> <code>MyGPT4ALL</code>           \u2013            <p>A custom LLM class that integrates gpt4all models</p> </li> <li> <code>DuckDuckGo</code>           \u2013            <p>Initiates a chat session with DuckDuckGo AI.</p> </li> <li> <code>WebBing</code>           \u2013            <p>Reverse-engineered Bing/Edge Copilot via Web browser.</p> </li> <li> <code>WebOpenGPTs</code>           \u2013            <p>Reverse-engineered model via Web browser.</p> </li> <li> <code>WebPhind</code>           \u2013            <p>Reverse-engineered model via Web browser.</p> </li> <li> <code>WebOpenai</code>           \u2013            <p>Reverse-engineered Openai via Web browser.</p> </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM","title":"<code>G4FLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Interface to the G4F service.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str = 'gpt-4'</code>)           \u2013            <p>The model to use for the chat client.</p> </li> <li> <code>provider</code>               (<code>str = None</code>)           \u2013            <p>The provider to use for the chat client.</p> </li> <li> <code>api_key</code>               (<code>str = None</code>)           \u2013            <p>The API key to use for the chat client.</p> </li> <li> <code>kwargs</code>               (<code>dict = None</code>)           \u2013            <p>Additional parameters to pass to the g4f client.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            </li> <li> <code>provider</code>               (<code>Union[str, ProviderType]</code>)           \u2013            </li> <li> <code>api_key</code>               (<code>str</code>)           \u2013            </li> <li> <code>create_kwargs</code>               (<code>dict[str, Any]</code>)           \u2013            </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.model","title":"<code>model: str = 'gpt-4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.provider","title":"<code>provider: Union[str, ProviderType] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.api_key","title":"<code>api_key: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.create_kwargs","title":"<code>create_kwargs: dict[str, Any] = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL","title":"<code>MyGPT4ALL(model_folder_path, model_name, allow_download, allow_streaming)</code>","text":"<p>               Bases: <code>LLM</code></p> <p>A custom LLM class that integrates gpt4all models</p> <p>Arguments:</p> <p>model_folder_path: (str) Folder path where the model lies model_name: (str) The name of the model to use (.bin) allow_download: (bool) whether to download the model or not allow_streaming: (bool) Whether to stream tokens or not <p>backend: (str) The backend of the model (Supported backends: llama/gptj) n_threads: (str) The number of threads to use n_predict: (str) The maximum numbers of tokens to generate temp: (str) Temperature to use for sampling top_p: (float) The top-p value to use for sampling top_k: (float) The top k values use for sampling n_batch: (int) Batch size for prompt processing repeat_last_n: (int) Last n number of tokens to penalize repeat_penalty: (float) The penalty to apply repeated tokens</p> <p>Methods:</p> <ul> <li> <code>auto_download</code>             \u2013              <p>This method will download the model to the specified path</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>model_folder_path</code>               (<code>str</code>)           \u2013            </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            </li> <li> <code>allow_download</code>               (<code>bool</code>)           \u2013            </li> <li> <code>allow_streaming</code>               (<code>bool</code>)           \u2013            </li> <li> <code>backend</code>               (<code>Optional[str]</code>)           \u2013            </li> <li> <code>temp</code>               (<code>Optional[float]</code>)           \u2013            </li> <li> <code>top_p</code>               (<code>Optional[float]</code>)           \u2013            </li> <li> <code>top_k</code>               (<code>Optional[int]</code>)           \u2013            </li> <li> <code>n_batch</code>               (<code>Optional[int]</code>)           \u2013            </li> <li> <code>n_threads</code>               (<code>Optional[int]</code>)           \u2013            </li> <li> <code>n_predict</code>               (<code>Optional[int]</code>)           \u2013            </li> <li> <code>max_tokens</code>               (<code>Optional[int]</code>)           \u2013            </li> <li> <code>repeat_last_n</code>               (<code>Optional[int]</code>)           \u2013            </li> <li> <code>repeat_penalty</code>               (<code>Optional[float]</code>)           \u2013            </li> <li> <code>gpt4_model_instance</code>               (<code>Any</code>)           \u2013            </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.model_folder_path","title":"<code>model_folder_path: str = model_folder_path</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.model_name","title":"<code>model_name: str = model_name</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.allow_download","title":"<code>allow_download: bool = allow_download</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.allow_streaming","title":"<code>allow_streaming: bool = allow_streaming</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.backend","title":"<code>backend: Optional[str] = 'llama'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.temp","title":"<code>temp: Optional[float] = 0.7</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.top_p","title":"<code>top_p: Optional[float] = 0.1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.top_k","title":"<code>top_k: Optional[int] = 40</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.n_batch","title":"<code>n_batch: Optional[int] = 8</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.n_threads","title":"<code>n_threads: Optional[int] = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.n_predict","title":"<code>n_predict: Optional[int] = 256</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.max_tokens","title":"<code>max_tokens: Optional[int] = 200</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.repeat_last_n","title":"<code>repeat_last_n: Optional[int] = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.repeat_penalty","title":"<code>repeat_penalty: Optional[float] = 1.18</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.gpt4_model_instance","title":"<code>gpt4_model_instance: Any = GPT4All(model_name=self.model_name, model_path=self.model_folder_path)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.auto_download","title":"<code>auto_download() -&gt; None</code>","text":"<p>This method will download the model to the specified path reference: python.langchain.com/docs/modules/model_io/models/llms/integrations/gpt4all</p>"},{"location":"llm_langchain/#thml.llm_langchain.DuckDuckGo","title":"<code>DuckDuckGo</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Initiates a chat session with DuckDuckGo AI.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model to use: \"gpt-3.5\", \"claude-3-haiku\". Defaults to \"gpt-3.5\".</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.DuckDuckGo.model","title":"<code>model: str = 'gpt-3.5'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebBing","title":"<code>WebBing</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered Bing/Edge Copilot via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>bot</code>               (<code>object</code>)           \u2013            <p>The Edge Chatbot object.</p> </li> <li> <code>conversation_style</code>               (<code>str</code>)           \u2013            <p>The conversation style. Available options: 'creative', 'balanced', 'precise'</p> </li> </ul> <p>NOTE: LangChain does not allow <code>__init__</code> method in the CustomLLM class. So, don't define any function in class-variables, since it will run when import module. All the running parts should be defined in the <code>_call</code> method. However, it will slow down the response time. Therefore, just pass critical parts to the <code>_call</code> method, all others should be defined outside this class.</p> <p>Attributes:</p> <ul> <li> <code>bot</code>               (<code>object</code>)           \u2013            </li> <li> <code>conversation_style</code>           \u2013            </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.WebBing.bot","title":"<code>bot: object</code>  <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebBing.conversation_style","title":"<code>conversation_style = 'precise'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenGPTs","title":"<code>WebOpenGPTs</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered model via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>The maximum number of tokens to generate. Default is 4096.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            </li> <li> <code>timeout</code>               (<code>int</code>)           \u2013            </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.WebOpenGPTs.max_tokens","title":"<code>max_tokens: int = 4096</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenGPTs.timeout","title":"<code>timeout: int = 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebPhind","title":"<code>WebPhind</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered model via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>The maximum number of tokens to generate. Default is 4096.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            </li> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            </li> <li> <code>timeout</code>               (<code>int</code>)           \u2013            </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.WebPhind.model","title":"<code>model: str = 'Phind Model'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebPhind.max_tokens","title":"<code>max_tokens: int = 4096</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebPhind.timeout","title":"<code>timeout: int = 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai","title":"<code>WebOpenai</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered Openai via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>bot</code>               (<code>object</code>)           \u2013            <p>The Edge Chatbot object.</p> </li> <li> <code>conversation_style</code>               (<code>str</code>)           \u2013            <p>The conversation style. Available options: 'creative', 'balanced', 'precise'</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>)           \u2013            </li> <li> <code>model</code>               (<code>str</code>)           \u2013            </li> <li> <code>keep_history</code>               (<code>bool</code>)           \u2013            </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai.cookie_file","title":"<code>cookie_file: str</code>  <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai.model","title":"<code>model: str = 'gpt-3.5'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai.keep_history","title":"<code>keep_history: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"rag/","title":"rag","text":""},{"location":"rag/#thml.rag","title":"<code>thml.rag</code>","text":"<p>Classes:</p> <ul> <li> <code>RAG</code>           \u2013            <p>Retrieval Augmented Generation (RAG) system.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>embedding_model</code>             \u2013              <p>Define the embedding function to use. See the list of available models of Langchain</p> </li> <li> <code>llm_model</code>             \u2013              <p>Predefined LangChain's style LLM models to be used in the RAG system.</p> </li> <li> <code>load_document</code>             \u2013              <p>Load documents from the given path, using langchain's [document_loaders].</p> </li> </ul>"},{"location":"rag/#thml.rag.RAG","title":"<code>RAG(rag_path: str = '', doc_path: str = None, llm: object = None, embedding: object = None, text_splitter: object = None, db: object = None, rerank: bool = False, style: str = 'simple')</code>","text":"<p>Retrieval Augmented Generation (RAG) system. Support vectorstore types: 'FAISS' or 'Chroma'. Default is 'FAISS'.</p> <p>Use reranker to improve the quality of the retrieved documents. Default is False. Note that the reranker model must different from the embedding model.</p> <p>Initialize the RAG system.</p> <p>Methods:</p> <ul> <li> <code>ask</code>             \u2013              <p>Perform an question-answering task and generate the answer to the given query that content come from the documents.</p> </li> <li> <code>ask_llm</code>             \u2013              <p>Ask the LLM model to generate the answer to the given query. It is different from <code>qa</code> function, which just return the answer if the documents contain the information. This function will generate the answer from the LLM model.</p> </li> <li> <code>search</code>             \u2013              <p>Search information from the documents. This is actually perform <code>retriever.invoke</code> to retrieve information from <code>vectorstore</code>. Refer: https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-retrieve.</p> </li> <li> <code>set_retriever</code>             \u2013              <p>Define parameters for the retriever. See <code>vectorstore.as_retriever</code> for more information.</p> </li> <li> <code>set_chain</code>             \u2013              <p>Set the style of the RAG system. The style can be 'simple', 'multi_query', or 'fusion'.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>embedding</code>           \u2013            </li> <li> <code>db</code>           \u2013            </li> <li> <code>text_splitter</code>           \u2013            </li> <li> <code>retriever</code>           \u2013            </li> <li> <code>compressor</code>           \u2013            </li> <li> <code>compression_retriever</code>           \u2013            </li> <li> <code>llm</code>           \u2013            </li> <li> <code>info</code>           \u2013            </li> </ul>"},{"location":"rag/#thml.rag.RAG.embedding","title":"<code>embedding = embedding</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.db","title":"<code>db = db</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.text_splitter","title":"<code>text_splitter = text_splitter</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.retriever","title":"<code>retriever = self.set_retriever(search_type='similarity', search_kwargs={'k': 6})</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.compressor","title":"<code>compressor = reranker_model()</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.compression_retriever","title":"<code>compression_retriever = ContextualCompressionRetriever(base_compressor=self.compressor, base_retriever=self.retriever)</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.llm","title":"<code>llm = llm</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.info","title":"<code>info</code>  <code>property</code>","text":""},{"location":"rag/#thml.rag.RAG.ask","title":"<code>ask(question='what are the documents about?') -&gt; str</code>","text":"<p>Perform an question-answering task and generate the answer to the given query that content come from the documents.</p> <p>Refer: https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-retrieve</p>"},{"location":"rag/#thml.rag.RAG.ask_llm","title":"<code>ask_llm(question='Who are you?') -&gt; str</code>","text":"<p>Ask the LLM model to generate the answer to the given query. It is different from <code>qa</code> function, which just return the answer if the documents contain the information. This function will generate the answer from the LLM model.</p>"},{"location":"rag/#thml.rag.RAG.search","title":"<code>search(question: str = 'summary') -&gt; list[Document]</code>","text":"<p>Search information from the documents. This is actually perform <code>retriever.invoke</code> to retrieve information from <code>vectorstore</code>. Refer: https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-retrieve.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query to search for.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>The number of documents to return.</p> </li> </ul> <p>Returns:     results (list[Document]): The documents that match the query.</p>"},{"location":"rag/#thml.rag.RAG.set_retriever","title":"<code>set_retriever(search_type: str = 'similarity', search_kwargs: dict = None)</code>","text":"<p>Define parameters for the retriever. See <code>vectorstore.as_retriever</code> for more information. Ref: https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore</p> <p>Parameters:</p> <ul> <li> <code>search_type</code>               (<code>Optional[str]</code>, default:                   <code>'similarity'</code> )           \u2013            <p>Defines the type of search that the Retriever should perform. Can be \"similarity\" (default), \"mmr\", or</p> </li> <li> <code>search_kwargs</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments to pass to the search function. Can include things like: k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold for similarity_score_threshold fetch_k: Amount of documents to pass to MMR algorithm (Default: 20) lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5) filter: Filter by document metadata</p> </li> </ul>"},{"location":"rag/#thml.rag.RAG.set_chain","title":"<code>set_chain(style: str = 'simple') -&gt; None</code>","text":"<p>Set the style of the RAG system. The style can be 'simple', 'multi_query', or 'fusion'.</p>"},{"location":"rag/#thml.rag.embedding_model","title":"<code>embedding_model(provider: str = 'huggingface', model_name: str = None, model_kwargs: dict = None) -&gt; object</code>","text":"<p>Define the embedding function to use. See the list of available models of Langchain</p> <p>Check the latest performance benchmarks for text embedding models at MTEB leaderboards hosted by Hugging Face. The fields to consider are:     - Score: the score we should focus on is \"average\" and \"retrieval average\". Both are highly correlated, so focusing on either works.     - Sequence length tells us how many tokens a model can consume and compress into a single embedding. Generally speaking, we wouldn't recommend stuffing more than a paragraph of heft into a single embedding - so models supporting up to 512 tokens are usually more than enough.     - Model size: the size of a model indicates how easy it will be to run. All models near the top of MTEB are reasonably sized. One of the largest is instructor-xl (requiring 4.96GB of memory), which we can easily run on consumer hardware.</p> Note <ul> <li>Embedding model may be referred to as SentenceTransformer in HF.</li> </ul> <p>Some HF's embedding models:     - mixedbread-ai/mxbai-embed-large-v1     - BAAI/bge-large-en-v1.5</p> <p>Parameters:</p> <ul> <li> <code>provider</code>               (<code>str</code>, default:                   <code>'huggingface'</code> )           \u2013            <p>The provider of the embeddings.</p> </li> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model to use for the reranker.</p> </li> <li> <code>model_kwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The parameters to use for the reranker.</p> </li> </ul>"},{"location":"rag/#thml.rag.llm_model","title":"<code>llm_model(service: str = 'web_opengpts', **kwargs: dict) -&gt; LLM</code>","text":"<p>Predefined LangChain's style LLM models to be used in the RAG system. Args:     service (str): The LLM model service. Available options: 'openai', 'web_openai', 'web_opengpts', 'web_phind', 'web_llama2', 'web_bing'     **kwargs: The model parameters, depend on the service. Returns:     LLM: The LangChain's LLM.</p>"},{"location":"rag/#thml.rag.load_document","title":"<code>load_document(doc_path: str = '', ext: str = None) -&gt; list[Document]</code>","text":"<p>Load documents from the given path, using langchain's [document_loaders]. Supported file types: .pdf, .docx, .txt, .md, .lnk (Windows' shortcuts).</p> <p>Parameters:</p> <ul> <li> <code>doc_path</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The path to the folder containing the documents.</p> </li> <li> <code>ext</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The file extension of the documents to be loaded, e.g., '.pdf'. Default, loads all files in the folder.</p> </li> </ul> <p>Returns:     list[Document]: A list of Document objects, containing the loaded documents.</p>"},{"location":"search_jobs/","title":"Search jobs","text":""},{"location":"search_jobs/#thml.search.jobs","title":"<code>thml.search.jobs</code>","text":"<p>Classes:</p> <ul> <li> <code>Indeed</code>           \u2013            </li> <li> <code>SimplyHired</code>           \u2013            </li> <li> <code>LinkedIn</code>           \u2013            </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.Indeed","title":"<code>Indeed(country: str = 'US')</code>","text":"<p>               Bases: <code>_JobBase</code></p> <p>Methods:</p> <ul> <li> <code>search_jobs</code>             \u2013              <p>Search jobs on the website</p> </li> <li> <code>get_job_single_page</code>             \u2013              <p>Get jobs from search result on single page</p> </li> <li> <code>get_job_multi_pages</code>             \u2013              <p>Get jobs from multi pages</p> </li> <li> <code>get_job_detail</code>             \u2013              <p>Get details of jobs from job lists</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>max_jobs</code>           \u2013            </li> <li> <code>jobs</code>           \u2013            </li> <li> <code>country</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.Indeed.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.max_jobs","title":"<code>max_jobs = 100</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.jobs","title":"<code>jobs = []</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.country","title":"<code>country = country</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.base_url","title":"<code>base_url = 'https://www.indeed.com/stc'</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.search_jobs","title":"<code>search_jobs(search_string: str = 'Molecular Dynamics Simulation', location: str = 'United States', filters: dict = None, detail: bool = False, max_jobs: int = 100)</code>","text":"<p>Search jobs on the website</p> <p>Parameters:</p> <ul> <li> <code>search_string</code>               (<code>str</code>, default:                   <code>'Molecular Dynamics Simulation'</code> )           \u2013            <p>a string to search for jobs</p> </li> <li> <code>location</code>               (<code>str</code>, default:                   <code>'United States'</code> )           \u2013            <p>location to search for jobs. A string in form \"city, state, zip\", or \"remote\"</p> </li> <li> <code>filters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>filters to apply. Available keys: \"distance\", \"job_type\", \"min_salary\", \"date_posted\"</p> </li> <li> <code>detail</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>get job details or not</p> </li> <li> <code>max_jobs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>maximum number of jobs to get</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>list[dict]: list of job items</p> </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.Indeed.get_job_single_page","title":"<code>get_job_single_page() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from search result on single page</p>"},{"location":"search_jobs/#thml.search.jobs.Indeed.get_job_multi_pages","title":"<code>get_job_multi_pages() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from multi pages</p>"},{"location":"search_jobs/#thml.search.jobs.Indeed.get_job_detail","title":"<code>get_job_detail() -&gt; list</code>  <code>async</code>","text":"<p>Get details of jobs from job lists</p>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired","title":"<code>SimplyHired(country: str = 'US')</code>","text":"<p>               Bases: <code>_JobBase</code></p> <p>Methods:</p> <ul> <li> <code>search_jobs</code>             \u2013              <p>Search jobs on the website</p> </li> <li> <code>get_job_single_page</code>             \u2013              <p>Get jobs from search result on single page</p> </li> <li> <code>get_job_multi_pages</code>             \u2013              <p>Get jobs from multi pages</p> </li> <li> <code>get_job_detail</code>             \u2013              <p>Get details of jobs from job lists</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>max_jobs</code>           \u2013            </li> <li> <code>jobs</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>jobperpage</code>           \u2013            </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.max_jobs","title":"<code>max_jobs = 100</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.jobs","title":"<code>jobs = []</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.base_url","title":"<code>base_url = 'https://www.simplyhired.com/'</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.jobperpage","title":"<code>jobperpage = 20</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.search_jobs","title":"<code>search_jobs(search_string: str = 'Molecular Dynamics Simulation', location: str = 'United States', filters: dict = None, detail: bool = False, max_jobs: int = 100)</code>","text":"<p>Search jobs on the website</p> <p>Parameters:</p> <ul> <li> <code>search_string</code>               (<code>str</code>, default:                   <code>'Molecular Dynamics Simulation'</code> )           \u2013            <p>a string to search for jobs</p> </li> <li> <code>location</code>               (<code>str</code>, default:                   <code>'United States'</code> )           \u2013            <p>location to search for jobs. A string in form \"city, state, zip\", or \"remote\"</p> </li> <li> <code>filters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>filters to apply. Available keys: \"distance\", \"job_type\", \"min_salary\", \"date_posted\"</p> </li> <li> <code>detail</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>get job details or not</p> </li> <li> <code>max_jobs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>maximum number of jobs to get</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>list[dict]: list of job items</p> </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.get_job_single_page","title":"<code>get_job_single_page() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from search result on single page</p>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.get_job_multi_pages","title":"<code>get_job_multi_pages() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from multi pages</p>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.get_job_detail","title":"<code>get_job_detail() -&gt; list</code>  <code>async</code>","text":"<p>Get details of jobs from job lists</p>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn","title":"<code>LinkedIn(cookie_file: str = None)</code>","text":"<p>               Bases: <code>_JobBase</code></p> <p>Methods:</p> <ul> <li> <code>search_jobs</code>             \u2013              <p>Search jobs on the website</p> </li> <li> <code>get_job_single_page</code>             \u2013              <p>Get jobs from search result on single page</p> </li> <li> <code>get_job_multi_pages</code>             \u2013              <p>Get jobs from multi pages</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>browser_kwargs</code>           \u2013            </li> <li> <code>context_kwargs</code>           \u2013            </li> <li> <code>page</code>           \u2013            </li> <li> <code>max_jobs</code>           \u2013            </li> <li> <code>jobs</code>           \u2013            </li> <li> <code>base_url</code>           \u2013            </li> <li> <code>jobperpage</code>           \u2013            </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.max_jobs","title":"<code>max_jobs = 100</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.jobs","title":"<code>jobs = []</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.base_url","title":"<code>base_url = 'https://www.linkedin.com/jobs'</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.jobperpage","title":"<code>jobperpage = 25</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.search_jobs","title":"<code>search_jobs(search_string: str = 'Molecular Dynamics Simulation', location: str = 'United States', filters: dict = None, detail: bool = False, max_jobs: int = 100)</code>","text":"<p>Search jobs on the website</p> <p>Parameters:</p> <ul> <li> <code>search_string</code>               (<code>str</code>, default:                   <code>'Molecular Dynamics Simulation'</code> )           \u2013            <p>a string to search for jobs</p> </li> <li> <code>location</code>               (<code>str</code>, default:                   <code>'United States'</code> )           \u2013            <p>location to search for jobs. A string in form \"city, state, zip\", or \"remote\"</p> </li> <li> <code>filters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>filters to apply. Available keys: \"distance\", \"job_type\", \"min_salary\", \"date_posted\"</p> </li> <li> <code>detail</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>get job details or not</p> </li> <li> <code>max_jobs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>maximum number of jobs to get</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>list[dict]: list of job items</p> </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn.get_job_single_page","title":"<code>get_job_single_page() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from search result on single page</p>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn.get_job_multi_pages","title":"<code>get_job_multi_pages() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from multi pages</p>"},{"location":"search_news/","title":"Search news","text":""},{"location":"search_news/#thml.search.news","title":"<code>thml.search.news</code>","text":"<p>Classes:</p> <ul> <li> <code>GooNews</code>           \u2013            <p>A class that allows you to search for news articles using Google News.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>news_by_topic</code>             \u2013              <p>Generate news video by topic.</p> </li> <li> <code>news_generator</code>             \u2013              <p>Generate news videos for a list of topics.</p> </li> </ul>"},{"location":"search_news/#thml.search.news.GooNews","title":"<code>GooNews(language='en', country='US', max_results=100, period=None, start_date=None, end_date=None, exclude_websites=None, proxy=None)</code>","text":"<p>               Bases: <code>GNews</code></p> <p>A class that allows you to search for news articles using Google News.</p> <p>Parameters:</p> <ul> <li> <code>language</code>               (<code>str</code>, default:                   <code>'en'</code> )           \u2013            <p>The language in which to return results. Defaults to en</p> </li> <li> <code>country</code>               (<code>str</code>, default:                   <code>'US'</code> )           \u2013            <p>The country code of the country you want to get headlines for. Defaults to US</p> </li> <li> <code>max_results</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The maximum number of results to return. The default is 100. Defaults to 100</p> </li> <li> <code>period</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The period of time from which you want the news</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Date after which results must have been published</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Date before which results must have been published</p> </li> <li> <code>exclude_websites</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>A list of strings that indicate websites to exclude from results</p> </li> <li> <code>proxy</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The proxy parameter is a dictionary with a single key-value pair. The key is the</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_article</code>             \u2013              <p>Download an article from the specified URL, parse it, and return an article object.</p> </li> <li> <code>download_article_material</code>             \u2013              <p>Download the article's text, images, and videos to the specified directory.</p> </li> </ul>"},{"location":"search_news/#thml.search.news.GooNews.get_article","title":"<code>get_article(url)</code>","text":"<p>Download an article from the specified URL, parse it, and return an article object.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The URL of the article you wish to summarize.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The article as defined by package <code>newpaper4k</code>, see here: https://newspaper4k.readthedocs.io/en/latest/user_guide/api_reference.html.</p> </li> <li>           \u2013            <p>This is different from the previous implementation which used <code>newspaper3k</code>.</p> </li> </ul>"},{"location":"search_news/#thml.search.news.GooNews.download_article_material","title":"<code>download_article_material(url, output_dir='./article_material')</code>","text":"<p>Download the article's text, images, and videos to the specified directory.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The URL of the article you wish to download.</p> </li> <li> <code>output_dir</code>               (<code>str</code>, default:                   <code>'./article_material'</code> )           \u2013            <p>The directory to save the article's material to. Defaults to \"./article_material\".</p> </li> </ul>"},{"location":"search_news/#thml.search.news.news_by_topic","title":"<code>news_by_topic(topic: str)</code>","text":"<p>Generate news video by topic.</p> <p>Tasks: 1. Get google news by topic 2. Download article material (text, images, videos). May be only text available. 3. Search related videos (youtube, google,...) and download them. 4. Generate video</p>"},{"location":"search_news/#thml.search.news.news_generator","title":"<code>news_generator()</code>","text":"<p>Generate news videos for a list of topics.</p> <p>Tasks: Run <code>news_by_topic</code> for each topic in a given list.</p>"},{"location":"util/","title":"util","text":""},{"location":"util/#thml.util","title":"<code>thml.util</code>","text":""},{"location":"vid/","title":"vid","text":""},{"location":"vid/#thml.video","title":"<code>thml.video</code>","text":"<p>Modules:</p> <ul> <li> <code>download_youtube</code>           \u2013            </li> <li> <code>increase_follower_insta</code>           \u2013            </li> <li> <code>make_video</code>           \u2013            <p>Info</p> </li> <li> <code>read_text</code>           \u2013            </li> <li> <code>search_google</code>           \u2013            </li> <li> <code>search_image</code>           \u2013            </li> <li> <code>tts</code>           \u2013            </li> </ul>"},{"location":"vid/#thml.video.download_youtube","title":"<code>download_youtube</code>","text":"<p>Functions:</p> <ul> <li> <code>videos_from_channel</code>             \u2013              <p>get all video links from a channel in \"period\" days</p> </li> <li> <code>download_video</code>             \u2013              <p>Download videos from list of URLs</p> </li> <li> <code>download_srt_caption</code>             \u2013              <p>Download video caption in .srt format</p> </li> <li> <code>download_json_caption</code>             \u2013              <p>Download video caption in .json format</p> </li> </ul>"},{"location":"vid/#thml.video.download_youtube.videos_from_channel","title":"<code>videos_from_channel(channel_URL, period=1)</code>","text":"<p>get all video links from a channel in \"period\" days</p>"},{"location":"vid/#thml.video.download_youtube.download_video","title":"<code>download_video(URLs, only_video=False, only_audio=False)</code>","text":"<p>Download videos from list of URLs Args:     URLs (list of str): list of URL     only_video (bool = True): dowload video only     only_audio (bool = True): dowload audio only</p>"},{"location":"vid/#thml.video.download_youtube.download_srt_caption","title":"<code>download_srt_caption(url, lang='en', out_file='transcript_srt')</code>","text":"<p>Download video caption in .srt format Args:     download_cap (bool = False): download caption     lang (str = 'en'): language of caption</p> <p>Notes\"     Error now: https://github.com/pytube/pytube/issues/1085</p>"},{"location":"vid/#thml.video.download_youtube.download_json_caption","title":"<code>download_json_caption(URL, out_file='transcript_json.json')</code>","text":"<p>Download video caption in .json format</p>"},{"location":"vid/#thml.video.increase_follower_insta","title":"<code>increase_follower_insta</code>","text":"<p>Attributes:</p> <ul> <li> <code>session</code>           \u2013            </li> </ul>"},{"location":"vid/#thml.video.increase_follower_insta.session","title":"<code>session = InstaPy(username='dumuc111', password='5hieuthuan')</code>  <code>module-attribute</code>","text":""},{"location":"vid/#thml.video.make_video","title":"<code>make_video</code>","text":"<p>Info</p> <p>Video size for 1 minute video : https://video4change.org/the-basics-of-video-resolution/     - Ultra HD or 4K: 3840 x 2160     320 MB     - Full HD       : 1920x1080       149 MB     - HD            : 1280x720        105 MB     - SD            : 720x480         26  MB    </p> <p>Quote</p> <p>running path: https://stackoverflow.com/questions/3430372/how-do-i-get-the-full-path-of-the-current-files-directory</p> <p>Functions:</p> <ul> <li> <code>mknews_video_intro</code>             \u2013              <p>Make INTRO video</p> </li> <li> <code>mknews_video_outro</code>             \u2013              <p>Make OUTRO video</p> </li> <li> <code>mknews_audio</code>             \u2013              <p>create audio from file_text</p> </li> <li> <code>mknews_1_video</code>             \u2013              <p>Make a video with concept:</p> </li> <li> <code>mknews_lists_videos</code>             \u2013              <p>Make a videos in subfolder:</p> </li> <li> <code>set_bg_audio</code>             \u2013              <p>Set background_audio for video</p> </li> <li> <code>mknews_video_toc</code>             \u2013              <p>Make a TOC video:</p> </li> <li> <code>concate_audio_files</code>             \u2013              <p>concate a list of audios</p> </li> <li> <code>concate_video_files</code>             \u2013              <p>concate a list of videos</p> </li> <li> <code>add_logo_spokeman</code>             \u2013              <p>add Logo on videos</p> </li> <li> <code>split_video</code>             \u2013              <p>Split video into n parts</p> </li> <li> <code>speech_word_by_word</code>             \u2013              </li> <li> <code>speech_1_pair_lang</code>             \u2013              <p>Returns:</p> </li> <li> <code>speech_list_pair_lang</code>             \u2013              <p>Args:</p> </li> <li> <code>mkvid_1_pair_lang</code>             \u2013              <p>Make a video with concept:</p> </li> <li> <code>mkvid_list_pair_lang_from_df</code>             \u2013              <p>Args:</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.mknews_video_intro","title":"<code>mknews_video_intro(vid_size=(1280, 720), lang='vi', rate=150, bg_video='default', bg_audio='default', bg_audio_factor=0.3, out_file='vid_intro.mp4')</code>","text":"<p>Make INTRO video</p> <p>Parameters:</p> <ul> <li> <code>vid_size</code>               (<code>tuple</code>, default:                   <code>(1280, 720)</code> )           \u2013            <p>Video size.</p> </li> <li> <code>lang</code>               (<code>str</code>, default:                   <code>'vi'</code> )           \u2013            <p>language of news</p> </li> <li> <code>rate</code>               (<code>float</code>, default:                   <code>150</code> )           \u2013            <p>speed of voice</p> </li> <li> <code>bg_video</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>filenames of video/image background. Possible: 'default', 'filename'</p> </li> <li> <code>bg_audio</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>file name of audio background. Possible: 'default', 'filename'</p> </li> <li> <code>bg_audio_factor</code>               (<code>float</code>, default:                   <code>0.3</code> )           \u2013            <p>factor of backgroun audio with main voice.</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.mknews_video_outro","title":"<code>mknews_video_outro(vid_size=(1280, 720), lang='vi', rate=150, bg_video='default', bg_audio='default', bg_audio_factor=0.3, out_file='vid_outro.mp4')</code>","text":"<p>Make OUTRO video Args:     vid_size (tuple): Video size.     lang (str): language of news     rate (float): speed of voice     bg_video (str): filenames of video/image background. Possible: 'default', 'filename'     bg_audio (str): file name of audio background. Possible: 'default', 'filename'     bg_audio_factor (float): factor of backgroun audio with main voice.</p>"},{"location":"vid/#thml.video.make_video.mknews_audio","title":"<code>mknews_audio(file_text, lang='vi', rate=150, greet_word='', end_word='', out_file='audio_news.mp3')</code>","text":"<p>create audio from file_text</p> <p>Parameters:</p> <ul> <li> <code>file_text</code>               (<code>str</code>)           \u2013            <p>lain text file.</p> </li> <li> <code>lang</code>               (<code>str</code>, default:                   <code>'vi'</code> )           \u2013            <p>language of news</p> </li> <li> <code>greet_word</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Add speech at begin text. Possible: 'intro', 'middle', ''</p> </li> <li> <code>end_word</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Add speech at begin text. Possible: 'outro', ''</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out_file</code> (              <code>Obj</code> )          \u2013            <p>audio file</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.mknews_1_video","title":"<code>mknews_1_video(lang='vi', rate=150, greet_word='', end_word='', vid_size=(1280, 720), img_duration=15, bg_video='', bg_audio='random', bg_audio_factor=0.2, out_file='vid_news.mp4')</code>","text":"Make a video with concept <ul> <li>put a text file and all videos, images into a folder</li> <li>function will convert text to audio</li> <li>make video base on length of audio</li> <li>first use videos, if not enough duration then add images into video</li> <li>if bg_video: make video with only background<ul> <li>random_short: use short videos from computer</li> <li>random_download: random download long videos from a predefined list</li> </ul> </li> </ul> <p>Parameters:</p> <ul> <li> <code>lang</code>               (<code>str</code>, default:                   <code>'vi'</code> )           \u2013            <p>language of news</p> </li> <li> <code>rate(float)</code>           \u2013            <p>speed of voice</p> </li> <li> <code>vid_size</code>               (<code>tuple</code>, default:                   <code>(1280, 720)</code> )           \u2013            <p>Video size.</p> </li> <li> <code>img_duration</code>               (<code>float</code>, default:                   <code>15</code> )           \u2013            <p>duration of an image in video.</p> </li> <li> <code>bg_video</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>filenames of video/image background. Possible: '', 'random_short', 'random_long'</p> </li> <li> <code>bg_audio</code>               (<code>str</code>, default:                   <code>'random'</code> )           \u2013            <p>file name of audio background. Possible: \"filename\", 'random'</p> </li> <li> <code>bg_audio_factor</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>factor of backgroun audio with main voice.</p> </li> <li> <code>greet_word</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Add speech at begin text. Possible: 'intro', 'middle', ''</p> </li> <li> <code>end_word</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Add speech at begin text. Possible: 'outro', ''</p> </li> </ul> Note <p>video/image files should begin with a number to specify its order: '1_video_...' or '3_image_...' Only fist \".txt\" files is used</p>"},{"location":"vid/#thml.video.make_video.mknews_lists_videos","title":"<code>mknews_lists_videos(sub_folder='news*', lang='vi', rate=150, greet_word='', end_word='', vid_size=(1280, 720), img_duration=15, bg_video='', bg_audio='', bg_audio_factor=0.2, padding=0, logo='STV', logo_pos='left', out_file='vid_all_news.mp4')</code>","text":"<p>Make a videos in subfolder:</p> <p>Parameters:</p> <ul> <li> <code>sub_folder</code>               (<code>str</code>, default:                   <code>'news*'</code> )           \u2013            <p>keyword to search subfolders.   </p> </li> <li> <code>bg_audio</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>file name of audio background. Possible: \"filename\", 'random'</p> </li> <li> <code>padding</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>gap between successive video</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.set_bg_audio","title":"<code>set_bg_audio(file_video, bg_audio='random', bg_audio_factor=0.2, keep_original=False)</code>","text":"<p>Set background_audio for video</p> <p>Parameters:</p> <ul> <li> <code>bg_audio</code>               (<code>str</code>, default:                   <code>'random'</code> )           \u2013            <p>file name of audio background. Possible: \"filename\", 'random'</p> </li> <li> <code>bg_audio_factor</code>               (<code>float</code>, default:                   <code>0.2</code> )           \u2013            <p>factor of backgroun audio with main voice.</p> </li> <li> <code>keep_original</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>keep original video or not</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.mknews_video_toc","title":"<code>mknews_video_toc(sub_folder='news*', file_video_news='vid_news.mp4', vid_size=(1280, 720), bg_video='default', bg_audio='default', bg_audio_factor=0.3, border_factor=0.2, with_title=False, out_file='vid_TOC.mp4')</code>","text":"<p>Make a TOC video:</p> <p>Parameters:</p> <ul> <li> <code>sub_folder</code>               (<code>str</code>, default:                   <code>'news*'</code> )           \u2013            <p>keyword to search subfolders. </p> </li> <li> <code>file_video_news</code>               (<code>str</code>, default:                   <code>'vid_news.mp4'</code> )           \u2013            <p>filename of breakingNews in each subfolder.</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.concate_audio_files","title":"<code>concate_audio_files(list_files, padding=0, out_file='concate_audio.mp3')</code>","text":"<p>concate a list of audios</p> <p>Parameters:</p> <ul> <li> <code>list_files</code>               (<code>list</code>)           \u2013            <p>list contains all audio files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>file</code> (              <code>obj</code> )          \u2013            <p>audio file.</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.concate_video_files","title":"<code>concate_video_files(list_files, padding=0, out_file='concate_videoNews.mp4')</code>","text":"<p>concate a list of videos</p> <p>Parameters:</p> <ul> <li> <code>list_files</code>               (<code>list</code>)           \u2013            <p>list contains all video files.</p> </li> <li> <code>vid_size</code>               (<code>tuple</code>)           \u2013            <p>Video size.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>file</code> (              <code>obj</code> )          \u2013            <p>audio file.</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.add_logo_spokeman","title":"<code>add_logo_spokeman(file_video, vid_size=(1280, 720), logo='STV', logo_pos='left', spokeman='', spokeman_pos='left', h_spokeman=320, keep_original=False)</code>","text":"<p>add Logo on videos</p> <p>Parameters:</p> <ul> <li> <code>file_video</code>               (<code>str</code>)           \u2013            <p>video filename.</p> </li> <li> <code>vid_size</code>               (<code>tuple</code>, default:                   <code>(1280, 720)</code> )           \u2013            <p>Video size.</p> </li> <li> <code>logo</code>               (<code>str</code>, default:                   <code>'STV'</code> )           \u2013            <p>Put logo on video. Possible: \"N5_1\", \"N5_2\", 'X7', 'STV', \"\"</p> </li> <li> <code>logo_pos</code>               (<code>float</code>, default:                   <code>'left'</code> )           \u2013            <p>Position of logo. Possible: \"left\", \"rigt\"</p> </li> <li> <code>spokeman</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>Spokeman on video. Possible: '', 'Anonymous'</p> </li> <li> <code>h_spokeman</code>               (<code>float</code>, default:                   <code>320</code> )           \u2013            <p>height of Spokeman</p> </li> <li> <code>background</code>               (<code>str</code>)           \u2013            <p>file name of video/image background. Possible: \"filename\", 'random'</p> </li> <li> <code>bg_audio</code>               (<code>str</code>)           \u2013            <p>file name of audio background</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.split_video","title":"<code>split_video(video_file, n=3)</code>","text":"<p>Split video into n parts</p>"},{"location":"vid/#thml.video.make_video.speech_word_by_word","title":"<code>speech_word_by_word(text, lang='VN', rate=150, vol=1.0, audio_file='word_by_word.mp3')</code>","text":""},{"location":"vid/#thml.video.make_video.speech_1_pair_lang","title":"<code>speech_1_pair_lang(text1, text2, lang1='VN', lang2='EN', voice_name1='', voice_name2='', rate1=130, rate2=120, repeat_slow=60, repeat_fast=60, out_file='pair_lang_audio.mp3')</code>","text":"<p>Returns:</p> <ul> <li> <code>file</code> (              <code>file</code> )          \u2013            <p>audio file, if <code>out_file</code> is not <code>None</code>.</p> </li> <li> <code>clip_audio</code> (              <code>Obj</code> )          \u2013            <p>audio file, if <code>out_file</code> is <code>None</code>.</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.speech_list_pair_lang","title":"<code>speech_list_pair_lang(df, lang1='VN', lang2='EN', rate1=130, rate2=120, repeat_slow=60, repeat_fast=60, out_file='pair_lang_audio_all.mp3')</code>","text":"<p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>contains 2 columns for langs.</p> </li> </ul>"},{"location":"vid/#thml.video.make_video.mkvid_1_pair_lang","title":"<code>mkvid_1_pair_lang(text1, text2, lang1='VN', lang2='EN', voice_name1='', voice_name2='', rate1=130, rate2=120, repeat_slow=60, repeat_fast=60, vid_size=(1280, 720), font_size=80, text_color1='blue', text_color2='black', bg_color1='azure3', bg_color2='azure4', padding=1, show_flag=True, out_file='pair_lang.mp4')</code>","text":"<p>Make a video with concept:</p> <p>Args:</p>"},{"location":"vid/#thml.video.make_video.mkvid_list_pair_lang_from_df","title":"<code>mkvid_list_pair_lang_from_df(df, lang1='VN', lang2='EN', voice_name1='', voice_name2='', rate1=130, rate2=120, repeat_slow=60, repeat_fast=60, vid_size=(1280, 720), font_size=80, text_color1='blue', text_color2='black', bg_color1='azure4', bg_color2='CadetBlue4', padding=1, show_flag=True, out_file='vid_pair_lang.mp4')</code>","text":"<p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>contains 2 columns for langs.</p> </li> </ul>"},{"location":"vid/#thml.video.read_text","title":"<code>read_text</code>","text":"<p>Functions:</p> <ul> <li> <code>pre_text</code>             \u2013              <p>Load preTEXT</p> </li> <li> <code>read_text_pair</code>             \u2013              <p>read file that format as pair, separated by semicolon (:). </p> </li> <li> <code>read_text_column</code>             \u2013              <p>read file that format as pair, separated by semicolon (:). </p> </li> <li> <code>read_text_news</code>             \u2013              <p>Read text </p> </li> <li> <code>find_abbrev</code>             \u2013              <p>https://stackoverflow.com/questions/60738190/regular-expression-to-find-a-series-of-uppercase-words-in-a-string</p> </li> <li> <code>read_text_subtitle</code>             \u2013              <p>Read caption file, support format: json, rst</p> </li> <li> <code>json_caption_to_text</code>             \u2013              <p>transcript (dict): download by youtubesearchpython.Transcript[\"segments\"]</p> </li> <li> <code>json_caption_unify</code>             \u2013              <p>transcript (dict): download by youtubesearchpython.Transcript[\"segments\"]</p> </li> <li> <code>convert_json_to_srt</code>             \u2013              <p>transcript (dict): download by youtubesearchpython.Transcript[\"segments\"]</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>DATA_PATH</code>           \u2013            </li> </ul>"},{"location":"vid/#thml.video.read_text.DATA_PATH","title":"<code>DATA_PATH = os.path.dirname(os.path.abspath(__file__)) + '/data'</code>  <code>module-attribute</code>","text":""},{"location":"vid/#thml.video.read_text.pre_text","title":"<code>pre_text(filename=None)</code>","text":"<p>Load preTEXT</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>Str</code>, default:                   <code>None</code> )           \u2013            <p>name of preTEXT file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ds</code> (              <code>Series</code> )          \u2013            <p>Series</p> </li> </ul>"},{"location":"vid/#thml.video.read_text.read_text_pair","title":"<code>read_text_pair(filename, separator=':')</code>","text":"<p>read file that format as pair, separated by semicolon (:). </p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>Str</code>)           \u2013            <p>name of text file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>df</code> (              <code>DataFrame</code> )          \u2013            <p>DataFrame contains 2 columns <code>c1</code> and <code>c2</code>, corresponds to pair text</p> </li> </ul>"},{"location":"vid/#thml.video.read_text.read_text_column","title":"<code>read_text_column(filename, separator=':', column_line=0)</code>","text":"<p>read file that format as pair, separated by semicolon (:). </p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>Str</code>)           \u2013            <p>name of text file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>df</code> (              <code>DataFrame</code> )          \u2013            <p>DataFrame contains 2 columns <code>c1</code> and <code>c2</code>, corresponds to pair text</p> </li> </ul>"},{"location":"vid/#thml.video.read_text.read_text_news","title":"<code>read_text_news(file_text, whole_text=False, replace_Abbrev=False, file_list_abbrev=DATA_PATH + '/list_abbrev.txt')</code>","text":"<p>Read text  Args:     file_text (str): plain text file.     whole_text (bool): read whole text and not decompose author, title,...     replace_Abbrev (bool): cho0se to auto replace Abbrev     file_list_abbrev (str): filename of .json file, contains list of Abbrev</p> <p>Note</p> <p>no empty line between 1<sup>st</sup> and 2<sup>nd</sup> lines</p>"},{"location":"vid/#thml.video.read_text.find_abbrev","title":"<code>find_abbrev(text)</code>","text":"<p>https://stackoverflow.com/questions/60738190/regular-expression-to-find-a-series-of-uppercase-words-in-a-string</p>"},{"location":"vid/#thml.video.read_text.read_text_subtitle","title":"<code>read_text_subtitle(filename, format_=None)</code>","text":"<p>Read caption file, support format: json, rst</p>"},{"location":"vid/#thml.video.read_text.json_caption_to_text","title":"<code>json_caption_to_text(transcript_dict, out_file=None)</code>","text":"<p>transcript (dict): download by youtubesearchpython.Transcript[\"segments\"]</p>"},{"location":"vid/#thml.video.read_text.json_caption_unify","title":"<code>json_caption_unify(transcript_dict)</code>","text":"<p>transcript (dict): download by youtubesearchpython.Transcript[\"segments\"]</p>"},{"location":"vid/#thml.video.read_text.convert_json_to_srt","title":"<code>convert_json_to_srt(transcript_dict, fps=25, out_file='transcript_rst.rst')</code>","text":"<p>transcript (dict): download by youtubesearchpython.Transcript[\"segments\"] fps (int): Frame per second. Can check using moviepy package</p>"},{"location":"vid/#thml.video.search_google","title":"<code>search_google</code>","text":"<p>Functions:</p> <ul> <li> <code>Distance</code>             \u2013              <p>Get google distance between words</p> </li> <li> <code>get_user_agent</code>             \u2013              <p>Get a random user agent string.</p> </li> <li> <code>get_hits</code>             \u2013              <p>This function return the amount of hits on search query</p> </li> <li> <code>Download</code>             \u2013              <p>Download url as html file</p> </li> <li> <code>search</code>             \u2013              <p>SEARCH</p> </li> </ul>"},{"location":"vid/#thml.video.search_google.Distance","title":"<code>Distance(term1, term2)</code>","text":"<p>Get google distance between words Returns float</p>"},{"location":"vid/#thml.video.search_google.get_user_agent","title":"<code>get_user_agent()</code>","text":"<p>Get a random user agent string. Return string</p>"},{"location":"vid/#thml.video.search_google.get_hits","title":"<code>get_hits(query, tld='com', lang='sv', tbs='0', safe='off', extra_params={}, tpe='', user_agent=None)</code>","text":"<p>This function return the amount of hits on search query Return int</p>"},{"location":"vid/#thml.video.search_google.Download","title":"<code>Download(url_list, out_format, download_dir)</code>","text":"<p>Download url as html file Returns folder</p>"},{"location":"vid/#thml.video.search_google.search","title":"<code>search(query, tld='com', lang='en', tbs='0', safe='off', num=10, start=0, stop=10, pause=2.0, only_standard=False, extra_params={}, tpe='', user_agent=None, type='text', rights='', download=False, download_dir='downloads', out_format='html')</code>","text":"<p>SEARCH This is a simplified search function implementation. I added some parameters to make it more generic towards google and google_search_image import. I have not experimented with all different parameters. Code assume from examples on the imported libraries github repos. ARGUMENTS:     query (str) \u2013 Query string. Must NOT be url-encoded.     tld (str) \u2013 Top level domain.     lang (str) \u2013 Language.     tbs (str) \u2013 Time limits (i.e \u201cqdr:h\u201d =&gt; last hour, \u201cqdr:d\u201d =&gt; last 24 hours, \u201cqdr:m\u201d =&gt; last month).     safe (str) \u2013 Safe search.     num (int) \u2013 Number of results per page.     start (int) \u2013 First result to retrieve. or None     stop (int) \u2013 Last result to retrieve. Use None to keep searching forever.     of str or None  (list) \u2013 A list of web  to constrain the search.     pause (float) \u2013 Lapse to wait between HTTP requests. A lapse too long will make the search slow, but a lapse too short may cause Google to block your IP. Your mileage may vary!     only_standard (bool) \u2013 If True, only returns the standard results from each page. If False, it returns every possible link from each page, except for those that point back to Google itself. Defaults to False for backwards compatibility with older versions of this module.     of str to str extra_params (dict) \u2013 A dictionary of extra HTTP GET parameters, which must be URL encoded. For example if you don\u2019t want Google to filter similar results you can set the extra_params to {\u2018filter\u2019: \u20180\u2019} which will append \u2018&amp;filter=0\u2019 to every query.     tpe (str) \u2013 Search type (images, videos, news, shopping, books, apps) Use the following values {videos: \u2018vid\u2019, images: \u2018isch\u2019, news: \u2018nws\u2019, shopping: \u2018shop\u2019, books: \u2018bks\u2019, applications: \u2018app\u2019}     or None user_agent (str) \u2013 User agent for the HTTP requests. Use None for the default.     type - Changes which function to use.     ----- For images only -----     download_dir -  if download is active, download_dir will discribe output directory     rights - (str) - Values labeled-for-reuse-with-modifications,labeled-for-reuse, labeled-for-noncommercial-reuse-with-modification,labeled-for-nocommercial-reuse     download - Download html, pdf or image,    Takes a set of urls and tries to download them to download_dir, If download_dir is None, won't save on drive, Return reference list to images, download_dir file to save to     # Read more here: https://python-googlesearch.readthedocs.io/en/latest/</p> <p>Returns:</p> <ul> <li>           \u2013            <p>Generator (iterator) that yields found URLs. If the stop parameter is None the iterator will loop forever.</p> </li> </ul>"},{"location":"vid/#thml.video.search_image","title":"<code>search_image</code>","text":"<p>Functions:</p> <ul> <li> <code>search_image</code>             \u2013              <p>Search and download images</p> </li> </ul>"},{"location":"vid/#thml.video.search_image.search_image","title":"<code>search_image(keywords, safe=False, download=False, num=10, pause=2.0, output_dir='download_image', time='past-7-days', time_range=None, rights='', similar_images=False, img_format=None, color=None, color_type=None, size='&gt;640*480', img_type=None, url=None, specific_site=None, single_image=None, ignore_urls=None)</code>","text":"<p>Search and download images</p> Agrs <p>keywords (str): Query string. Must NOT be url-encoded. tld (str) : Top level domain. format (str): format/extension of the image. Possible values: jpg, gif, png, bmp, svg, webp, ico, raw</p> <p>safe (str) : Safe search. num (int) : Number of results per page. start (int) : First result to retrieve. or None stop (int) : Last result to retrieve. Use None to keep searching forever. of str or None  (list) : A list of web  to constrain the search. pause (float) : Lapse to wait between HTTP requests. A lapse too long will make the search slow, but a lapse too short may cause Google to block your IP. Your mileage may vary! only_standard (bool) : If True, only returns the standard results from each page. If False, it returns every possible link from each page, except for those that point back to Google itself. Defaults to False for backwards compatibility with older versions of this module. of str to str extra_params (dict) : A dictionary of extra HTTP GET parameters, which must be URL encoded. For example if you don't want Google to filter similar results you can set the extra_params to {'filter': '0'} which will append '&amp;filter=0' to every query. tpe (str) : Search type (images, videos, news, shopping, books, apps) Use the following values {videos: 'vid', images: 'isch', news: 'nws', shopping: 'shop', books: 'bks', applications: 'app'} or None user_agent (str) : User agent for the HTTP requests. Use None for the default. type: Changes which function to use. output_dir:  if download is active, output_dir will discribe output directory  rights (str): Values labeled-for-reuse-with-modifications,labeled-for-reuse, labeled-for-noncommercial-reuse-with-modification,labeled-for-nocommercial-reuse download: Download html, pdf or image,    Takes a set of urls and tries to download them to output_dir, If output_dir is None, won't save on drive, Return reference list to images, output_dir file to save to</p> <p>Returns:</p> <ul> <li>           \u2013            <p>Generator (iterator) that yields found URLs. If the stop parameter is None the iterator will loop forever.</p> </li> </ul> Refs <ul> <li>Read more here: https://python-googlesearch.readthedocs.io/en/latest/</li> <li>List args: https://google-images-download.readthedocs.io/en/latest/arguments.html</li> </ul>"},{"location":"vid/#thml.video.tts","title":"<code>tts</code>","text":"<p>Functions:</p> <ul> <li> <code>tts_on_edge</code>             \u2013              <p>TTS using Edge browser</p> </li> <li> <code>tts_on_gTTS</code>             \u2013              <p>Args:</p> </li> <li> <code>tts_off_pyttsx3</code>             \u2013              <p>Convert text to speech using Windows' voices.</p> </li> </ul>"},{"location":"vid/#thml.video.tts.tts_on_edge","title":"<code>tts_on_edge(text: str, lang: str = 'vi', voice_name: str = 'vi-VN-HoaiMyNeural', audio_file: str = 'voice.mp3')</code>","text":"<p>TTS using Edge browser</p>"},{"location":"vid/#thml.video.tts.tts_on_gTTS","title":"<code>tts_on_gTTS(text, lang='en', audio_file='voice.mp3')</code>","text":"<p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>text string</p> </li> </ul>"},{"location":"vid/#thml.video.tts.tts_off_pyttsx3","title":"<code>tts_off_pyttsx3(text, lang='vi', voice_name='', voice_id=None, rate=150, vol=1.0, audio_file='voice.mp3')</code>","text":"<p>Convert text to speech using Windows' voices. Agrs:     lang (str): select the language. Possible with all voices available in local computer: 'vi', 'VN', 'US',...     text (str): string of text     rate (float): voice speed     voice_name (str): name of speaker.     voice_id (int): id of voices in Windows, this parameter sets both languages and voice.</p>"}]}