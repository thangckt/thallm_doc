{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"<code>thml</code> Documentation","text":""},{"location":"#thml","title":"<code>thml</code>","text":"<p>The package for LLM applications</p> <p>This package is developed and maintained by C.Thang Nguyen</p>"},{"location":"llm_chat/","title":"llm_chat","text":""},{"location":"llm_chat/#thml.llm_chat","title":"<code>thml.llm_chat</code>","text":"<p>This module contains the chatbot classes and methods to work with text generation models.</p>"},{"location":"llm_chat/#thml.llm_chat.post_api","title":"<code>post_api</code>","text":"<p>This module contains the chatbot classes and methods to work with text generation models.</p>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google","title":"<code>chat_google</code>","text":"<p>Build a chatbot using Google's Gemini API</p> <p>REF: - Python API: https://ai.google.dev/tutorials/python_quickstart - Prompt examples: https://ai.google.dev/docs/prompt_best_practices - Palm vs Gemini: https://ai.google.dev/docs/migration_guide</p>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google","title":"<code>Google(api_key=None, **kwargs)</code>","text":"<p>Class for chatbot using Google's Gemini API via google.generativeai package</p> <p>Parameters:</p> <ul> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The OpenAI API key.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>model</code>               (<code>str = 'gemini-pro'</code>)           \u2013            <p>The model to use for the chat client.</p> </li> <li> <code>temperature</code>               (<code>float = 0.7</code>)           \u2013            <p>The temperature to use for the chat client.</p> </li> <li> <code>top_p</code>               (<code>float = 1</code>)           \u2013            <p>An alternative to sampling with temperature.</p> </li> <li> <code>max_tokens</code>               (<code>int = 8096</code>)           \u2013            <p>The maximum number of tokens to generate in the response.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google.params","title":"<code>params = kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_google.Google.ask","title":"<code>ask(prompt='hello my friend')</code>","text":"<p>Ask Google Gemini a question and return the answer.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'hello my friend'</code> )           \u2013            <p>The question or prompt to ask the chatbot.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>text</code> (              <code>str</code> )          \u2013            <p>The answer to the question.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free","title":"<code>chat_gpt4free</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat","title":"<code>FreeChat(**kwargs: Any)</code>","text":"<p>               Bases: <code>_Base</code></p> <p>Class for chatbot using reverse enginerring models.</p> <p>Other Parameters:</p> <ul> <li> <code>provider</code>               (<code>str = None</code>)           \u2013            <p>The provider of the model. If None, the best provider will be used.</p> </li> <li> <code>api_key</code>               (<code>str = None</code>)           \u2013            <p>The API key for the provider.</p> </li> <li> <code>model</code>               (<code>str = 'gpt-4'</code>)           \u2013            <p>The model to use.</p> </li> <li> <code>temperature</code>               (<code>float = 0.7</code>)           \u2013            <p>The temperature of the model.</p> </li> <li> <code>top_p</code>               (<code>float = 1</code>)           \u2013            <p>The top_p of the model.</p> </li> <li> <code>max_tokens</code>               (<code>int = 8096</code>)           \u2013            <p>The max tokens of the model.</p> </li> <li> <code>system_prompt</code>               (<code>str = ''</code>)           \u2013            <p>The system prompt of the model.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.params","title":"<code>params = _params(**kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.avail_providers","title":"<code>avail_providers = self._avail_providers()</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeChat.ask","title":"<code>ask(prompt: str) -&gt; str</code>","text":"<p>Ask the chatbot a question. Args:     prompt (str): The input string for the chatbot.</p> <p>Returns:</p> <ul> <li> <code>text</code> (              <code>str</code> )          \u2013            <p>The answer to the question.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_gpt4free.FreeImage","title":"<code>FreeImage()</code>","text":"<p>Class for image generation using reverse-enginered models.</p>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai","title":"<code>chat_openai</code>","text":"<p>Using chatGPT API to build a chatbot</p> <p>Implementation following this repo: https://github.com/stancsz/chatgpt/blob/master/ChatGPT.py</p> <p>Terms: - OpenAI's text generation models (often called generative pre-trained transformers or large language models) - The inputs to these models are also referred to as \"prompts\".</p> <p>REF: - Refer to file: src_thatool\\devtools\\dev_chatGPT\\chat_API/chat_Copilot2GPT.ipynb - openai docs: https://platform.openai.com/docs/guides/text-generation/text-generation-models - openai repo: https://github.com/openai/openai-python - prompt examples: https://github.com/f/awesome-chatgpt-prompts - openai examples: https://platform.openai.com/examples - inherent class in python: https://stackoverflow.com/questions/9575409/calling-parent-class-init-with-multiple-inheritance-whats-the-right-way - super() in python: https://stackoverflow.com/questions/34550425/how-to-initialize-subclass-paramseters-in-python-using-super - create chatbot using openai API: https://medium.com/data-professor/beginners-guide-to-openai-api-a0420bc58ee5 - OpenAI API tips: https://arize.com/blog-course/mastering-openai-api-tips-and-tricks/</p>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat","title":"<code>BaseChat</code>","text":"<p>Base class for chatbot, to define common attributes and methods for chatbot</p>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat.save_history","title":"<code>save_history(prompt, response)</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat.export_history","title":"<code>export_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.BaseChat.load_history","title":"<code>load_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai","title":"<code>Openai(service: str = 'openai', **kwargs: Any)</code>","text":"<p>               Bases: <code>BaseChat</code></p> <p>Class for chatbot using OpenAI API via <code>openai</code> package</p> <p>Parameters:</p> <ul> <li> <code>service</code>               (<code>str</code>, default:                   <code>'openai'</code> )           \u2013            <p>The service to use for the chat client (preset of base_url). Available services are: openai, copilot, local_gpt4all</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>base_url</code>               (<code>str</code>)           \u2013            <p>The OpenAI API base URL. Presetted based on the service.</p> </li> <li> <code>api_key</code>               (<code>str</code>)           \u2013            <p>The OpenAI API key. Presetted based on the service.</p> </li> <li> <code>model</code>               (<code>str = 'gpt-4'</code>)           \u2013            <p>The model to use for the chat client. All models can be found at the OpenAI site. Only 2 models 'gpt-4' and 'gpt-3.5-turbo' for copilot.</p> </li> <li> <code>temperature</code>               (<code>float = 0.7</code>)           \u2013            <p>The temperature to use for the chat client. The temperature is a value between 0 and 1. Lower temperatures will cause the model to repeat itself more often, while higher temperatures will increase the model's diversity of responses. Use either <code>temperature</code> or <code>top_p</code>, but not both.</p> </li> <li> <code>top_p</code>               (<code>float = 1</code>)           \u2013            <p>An alternative to sampling with temperature. The top_p is a value between 0 and 1. Use either <code>temperature</code> or <code>top_p</code>, but not both.</p> </li> <li> <code>max_tokens</code>               (<code>int = 8096</code>)           \u2013            <p>The maximum number of tokens to generate in the response.</p> </li> <li> <code>stream</code>               (<code>bool = False</code>)           \u2013            <p>Whether to stream the response or not.</p> </li> <li> <code>system_prompt</code>               (<code>str = ''</code>)           \u2013            <p>The prompt to use for the system.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.params","title":"<code>params = _params(service, **kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.save_history","title":"<code>save_history(prompt, response)</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.export_history","title":"<code>export_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.load_history","title":"<code>load_history(filename='chat_history.txt')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Openai.ask","title":"<code>ask(prompt='hello', **kwargs: Any) -&gt; str</code>","text":"<p>Ask GPT-4 a question and return the answer. Use new openai API</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'hello'</code> )           \u2013            <p>The question to ask GPT-4.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>save_history</code>               (<code>bool = False</code>)           \u2013            <p>Whether to save the question and answer to the chat history.</p> </li> <li> <code>use_history</code>               (<code>bool = False</code>)           \u2013            <p>Whether to use the chat history in the current request.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>text</code> (              <code>str</code> )          \u2013            <p>The answer to the question.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post","title":"<code>Post(service: str = 'copilot', **kwargs: Any)</code>","text":"<p>Class for chatbot using OpenAI API via POST request</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>See <code>Openai</code> class for the arguments.</p> </li> </ul> Refs <ul> <li>https://www.techasoft.com/post/how-to-use-chatgpt-api-in-python-for-your-real-time-data)</li> <li>Curl vs python's requests: https://stackoverflow.com/questions/31061227/curl-vs-python-requests-when-hitting-apis</li> <li>Python and REST APIs: Interacting With Web Services: https://realpython.com/api-integration-in-python/</li> <li>Asynchronous Requests in Python: https://superfastpython.com/python-async-requests/</li> <li>Make request python faster: https://skillshats.com/blogs/optimize-python-requests-for-faster-performance/</li> </ul>"},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post.params","title":"<code>params = _params(service, **kwargs)</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post.avail_models","title":"<code>avail_models = avail_models</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.post_api.chat_openai.Post.ask","title":"<code>ask(prompt='hello')</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright","title":"<code>web_playwright</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt","title":"<code>chatgpt</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt","title":"<code>WebChatgpt(cookie_file: str = None, proxy: str = None, chat_id: str = 'temporary')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'temporary'</code> )           \u2013            <p>\"id\", \"last\", \"temporary\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_page=\"temporary\"</code>. If \"last\", use last chat. If \"temporary\", use temporary chat (need to go wedsite to turn on this feature at the first time login of an account). If \"new\", start new chat.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.base_url","title":"<code>base_url = 'https://chatgpt.com'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.login","title":"<code>login = False</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_placeholder('Message ChatGPT')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.send_button","title":"<code>send_button = self.page.locator('button.mb-1.me-1.h-8.w-8')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.stop_button","title":"<code>stop_button = self.page.locator('button.mb-1.me-1.h-8.w-8').locator('rect')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_all_messages","title":"<code>get_all_messages() -&gt; list[dict]</code>  <code>async</code>","text":"<p>Get all messages by user and AI</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.chatgpt.WebChatgpt.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude","title":"<code>claude</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude","title":"<code>WebClaude(cookie_file: str = None, proxy: str = None, chat_id: str = 'last')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'last'</code> )           \u2013            <p>\"id\", \"last\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_id=\"last\"</code>. If \"last\", use last_id. If \"new\", start new chat.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.base_url","title":"<code>base_url = 'https://claude.ai'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_label('Write your prompt to Claude').locator('p')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Send Message')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop Response')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.claude.WebClaude.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright","title":"<code>copilot_playwright</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot","title":"<code>WebCopilot(cookie_file: str = None, proxy: str = None, chat_id: str = None, converstion_style: str = None)</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.base_url","title":"<code>base_url = 'https://copilot.microsoft.com/'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_role('textbox', name='Ask me anything...')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Submit')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop Responding')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.upload_image_button","title":"<code>upload_image_button = self.page.get_by_role('button', name='Add an image to search')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.upload_file_button","title":"<code>upload_file_button = self.page.get_by_role('button', name='Add a file')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_last_ai_reference","title":"<code>get_last_ai_reference()</code>  <code>async</code>","text":"<p>Get the references in last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.copilot_playwright.WebCopilot.get_all_messages","title":"<code>get_all_messages() -&gt; list[dict]</code>  <code>async</code>","text":"<p>Get all messages by user and AI</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini","title":"<code>gemini</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini","title":"<code>WebGemini(cookie_file: str = None, proxy: str = None, conversation_id: str = None)</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.base_url","title":"<code>base_url = 'https://gemini.google.com/app'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_role('textbox')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.get_last_message","title":"<code>get_last_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.get_all_messages","title":"<code>get_all_messages() -&gt; list[dict]</code>  <code>async</code>","text":"<p>Get all messages by user and AI</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.gemini.WebGemini.up_load_file","title":"<code>up_load_file()</code>  <code>async</code>","text":"<p>Get text</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama","title":"<code>llama</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama","title":"<code>WebLlama(cookie_file: str = None, proxy: str = None, chat_id: str = 'last')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'last'</code> )           \u2013            <p>\"id\", \"last\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_id=\"last\"</code>. If \"last\", use last_id. If \"new\", start new chat.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.base_url","title":"<code>base_url = 'https://chatwithllama.com'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_placeholder('Ask anything!')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Send question')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop generation')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.llama.WebLlama.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral","title":"<code>mistral</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral","title":"<code>WebMistral(cookie_file: str = None, proxy: str = None, chat_id: str = 'last')</code>","text":"<p>               Bases: <code>WebBase</code></p> <p>Interacte with chatgpt web</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>path to cookie file</p> </li> <li> <code>proxy</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>proxy server. e.g., \"http://something.com:8080\"</p> </li> <li> <code>chat_id</code>               (<code>str</code>, default:                   <code>'last'</code> )           \u2013            <p>\"id\", \"last\", \"new\". If \"id\", use <code>chat_id</code>. If <code>chat_id</code> is not be found on the web, fallback to <code>chat_id=\"last\"</code>. If \"last\", use last_id. If \"new\", start new chat.</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.device","title":"<code>device: str = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.send_count","title":"<code>send_count = 0</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.base_url","title":"<code>base_url = 'https://chat.mistral.ai'</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.chat_id","title":"<code>chat_id = chat_id</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.prompt_textarea","title":"<code>prompt_textarea = self.page.get_by_placeholder('Ask anything!')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.send_button","title":"<code>send_button = self.page.get_by_role('button', name='Send question')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.stop_button","title":"<code>stop_button = self.page.get_by_role('button', name='Stop generation')</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.ask","title":"<code>ask(prompt: str = 'Hello, are you gpt-4o?')</code>","text":"<p>Ask and get reponse from web</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>response from AI</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_chat_history","title":"<code>get_chat_history()</code>","text":"<p>alias of get_all_messages() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.close","title":"<code>close()</code>","text":"<p>Alias of _close_page() method, but in sync mode</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.send_prompt","title":"<code>send_prompt(prompt: str = 'Hello, are you gpt-4o?')</code>  <code>async</code>","text":"<p>Submit prompt text</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>, default:                   <code>'Hello, are you gpt-4o?'</code> )           \u2013            <p>prompt text</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_last_ai_message","title":"<code>get_last_ai_message()</code>  <code>async</code>","text":"<p>Get the last AI message</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_all_chat_id","title":"<code>get_all_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_playwright.mistral.WebMistral.get_last_chat_id","title":"<code>get_last_chat_id()</code>  <code>async</code>","text":"<p>Get all conversation ids</p>"},{"location":"llm_chat/#thml.llm_chat.web_requests","title":"<code>web_requests</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot","title":"<code>bing_copilot</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot","title":"<code>RWebCopilot(cookie_file: str = None, conversation_style: Literal['creative', 'balanced', 'precise'] = 'precise')</code>","text":"<p>Reverse-engineered Bing/Edge Copilot via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>cookie_file</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cookie file.</p> </li> <li> <code>conversation_style</code>               (<code>str</code>, default:                   <code>'precise'</code> )           \u2013            <p>The conversation style. Available options: 'creative', 'balanced', 'precise'</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.cookie_file","title":"<code>cookie_file = cookie_file</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.bot","title":"<code>bot = asyncio.run(Chatbot.create(cookies=cookies))</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.conversation_style","title":"<code>conversation_style = style_map[conversation_style]</code>  <code>instance-attribute</code>","text":""},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.RWebCopilot.ask","title":"<code>ask(prompt: str, attachment: str = None, return_refs: bool = False) -&gt; Union[str, dict[str, list]]</code>","text":"<p>Ask the bot a question</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>str</code>)           \u2013            <p>The prompt to ask the bot</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[str, dict[str, list]]</code>           \u2013            <p>dict[str, list[str]]: final_text, references</p> </li> </ul>"},{"location":"llm_chat/#thml.llm_chat.web_requests.bing_copilot.response_parser","title":"<code>response_parser(response: dict) -&gt; dict[str, list[str]]</code>","text":"<p>Parse the response from the re_edge_gpt chatbot</p> <p>Parameters:</p> <ul> <li> <code>response</code>               (<code>dict</code>)           \u2013            <p>response from the re_edge_gpt chatbot</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list[str]]</code>           \u2013            <p>dict[str, list[str]]: final_text, references</p> </li> </ul>"},{"location":"llm_langchain/","title":"llm_langchain","text":""},{"location":"llm_langchain/#thml.llm_langchain","title":"<code>thml.llm_langchain</code>","text":"<p>Interface LLM models into LangChain's style to be used in the RAG system.</p>"},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM","title":"<code>G4FLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Interface to the G4F service.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str = 'gpt-4'</code>)           \u2013            <p>The model to use for the chat client.</p> </li> <li> <code>provider</code>               (<code>str = None</code>)           \u2013            <p>The provider to use for the chat client.</p> </li> <li> <code>api_key</code>               (<code>str = None</code>)           \u2013            <p>The API key to use for the chat client.</p> </li> <li> <code>kwargs</code>               (<code>dict = None</code>)           \u2013            <p>Additional parameters to pass to the g4f client.</p> </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.model","title":"<code>model: str = 'gpt-4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.provider","title":"<code>provider: Union[str, ProviderType] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.api_key","title":"<code>api_key: str = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.G4FLLM.create_kwargs","title":"<code>create_kwargs: dict[str, Any] = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL","title":"<code>MyGPT4ALL(model_folder_path, model_name, allow_download, allow_streaming)</code>","text":"<p>               Bases: <code>LLM</code></p> <p>A custom LLM class that integrates gpt4all models</p> <p>Arguments:</p> <p>model_folder_path: (str) Folder path where the model lies model_name: (str) The name of the model to use (.bin) allow_download: (bool) whether to download the model or not allow_streaming: (bool) Whether to stream tokens or not <p>backend: (str) The backend of the model (Supported backends: llama/gptj) n_threads: (str) The number of threads to use n_predict: (str) The maximum numbers of tokens to generate temp: (str) Temperature to use for sampling top_p: (float) The top-p value to use for sampling top_k: (float) The top k values use for sampling n_batch: (int) Batch size for prompt processing repeat_last_n: (int) Last n number of tokens to penalize repeat_penalty: (float) The penalty to apply repeated tokens</p>"},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.model_folder_path","title":"<code>model_folder_path: str = model_folder_path</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.model_name","title":"<code>model_name: str = model_name</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.allow_download","title":"<code>allow_download: bool = allow_download</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.allow_streaming","title":"<code>allow_streaming: bool = allow_streaming</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.backend","title":"<code>backend: Optional[str] = 'llama'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.temp","title":"<code>temp: Optional[float] = 0.7</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.top_p","title":"<code>top_p: Optional[float] = 0.1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.top_k","title":"<code>top_k: Optional[int] = 40</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.n_batch","title":"<code>n_batch: Optional[int] = 8</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.n_threads","title":"<code>n_threads: Optional[int] = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.n_predict","title":"<code>n_predict: Optional[int] = 256</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.max_tokens","title":"<code>max_tokens: Optional[int] = 200</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.repeat_last_n","title":"<code>repeat_last_n: Optional[int] = 64</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.repeat_penalty","title":"<code>repeat_penalty: Optional[float] = 1.18</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.gpt4_model_instance","title":"<code>gpt4_model_instance: Any = GPT4All(model_name=self.model_name, model_path=self.model_folder_path)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.MyGPT4ALL.auto_download","title":"<code>auto_download() -&gt; None</code>","text":"<p>This method will download the model to the specified path reference: python.langchain.com/docs/modules/model_io/models/llms/integrations/gpt4all</p>"},{"location":"llm_langchain/#thml.llm_langchain.DuckDuckGo","title":"<code>DuckDuckGo</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Initiates a chat session with DuckDuckGo AI.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model to use: \"gpt-3.5\", \"claude-3-haiku\". Defaults to \"gpt-3.5\".</p> </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.DuckDuckGo.model","title":"<code>model: str = 'gpt-3.5'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebBing","title":"<code>WebBing</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered Bing/Edge Copilot via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>bot</code>               (<code>object</code>)           \u2013            <p>The Edge Chatbot object.</p> </li> <li> <code>conversation_style</code>               (<code>str</code>)           \u2013            <p>The conversation style. Available options: 'creative', 'balanced', 'precise'</p> </li> </ul> <p>NOTE: LangChain does not allow <code>__init__</code> method in the CustomLLM class. So, don't define any function in class-variables, since it will run when import module. All the running parts should be defined in the <code>_call</code> method. However, it will slow down the response time. Therefore, just pass critical parts to the <code>_call</code> method, all others should be defined outside this class.</p>"},{"location":"llm_langchain/#thml.llm_langchain.WebBing.bot","title":"<code>bot: object</code>  <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebBing.conversation_style","title":"<code>conversation_style = 'precise'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenGPTs","title":"<code>WebOpenGPTs</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered model via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>The maximum number of tokens to generate. Default is 4096.</p> </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.WebOpenGPTs.max_tokens","title":"<code>max_tokens: int = 4096</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenGPTs.timeout","title":"<code>timeout: int = 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebPhind","title":"<code>WebPhind</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered model via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>max_tokens</code>               (<code>int</code>)           \u2013            <p>The maximum number of tokens to generate. Default is 4096.</p> </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.WebPhind.model","title":"<code>model: str = 'Phind Model'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebPhind.max_tokens","title":"<code>max_tokens: int = 4096</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebPhind.timeout","title":"<code>timeout: int = 60</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai","title":"<code>WebOpenai</code>","text":"<p>               Bases: <code>LLM</code></p> <p>Reverse-engineered Openai via Web browser.</p> <p>Parameters:</p> <ul> <li> <code>bot</code>               (<code>object</code>)           \u2013            <p>The Edge Chatbot object.</p> </li> <li> <code>conversation_style</code>               (<code>str</code>)           \u2013            <p>The conversation style. Available options: 'creative', 'balanced', 'precise'</p> </li> </ul>"},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai.cookie_file","title":"<code>cookie_file: str</code>  <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai.model","title":"<code>model: str = 'gpt-3.5'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"llm_langchain/#thml.llm_langchain.WebOpenai.keep_history","title":"<code>keep_history: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"rag/","title":"rag","text":""},{"location":"rag/#thml.rag","title":"<code>thml.rag</code>","text":""},{"location":"rag/#thml.rag.RAG","title":"<code>RAG(rag_path: str = '', doc_path: str = None, llm: object = None, embedding: object = None, text_splitter: object = None, db: object = None, rerank: bool = False, style: str = 'simple')</code>","text":"<p>Retrieval Augmented Generation (RAG) system. Support vectorstore types: 'FAISS' or 'Chroma'. Default is 'FAISS'.</p> <p>Use reranker to improve the quality of the retrieved documents. Default is False. Note that the reranker model must different from the embedding model.</p> <p>Initialize the RAG system.</p>"},{"location":"rag/#thml.rag.RAG.embedding","title":"<code>embedding = embedding</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.db","title":"<code>db = db</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.text_splitter","title":"<code>text_splitter = text_splitter</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.retriever","title":"<code>retriever = self.set_retriever(search_type='similarity', search_kwargs={'k': 6})</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.compressor","title":"<code>compressor = reranker_model()</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.compression_retriever","title":"<code>compression_retriever = ContextualCompressionRetriever(base_compressor=self.compressor, base_retriever=self.retriever)</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.llm","title":"<code>llm = llm</code>  <code>instance-attribute</code>","text":""},{"location":"rag/#thml.rag.RAG.info","title":"<code>info</code>  <code>property</code>","text":""},{"location":"rag/#thml.rag.RAG.ask","title":"<code>ask(question='what are the documents about?') -&gt; str</code>","text":"<p>Perform an question-answering task and generate the answer to the given query that content come from the documents.</p> <p>Refer: https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-retrieve</p>"},{"location":"rag/#thml.rag.RAG.ask_llm","title":"<code>ask_llm(question='Who are you?') -&gt; str</code>","text":"<p>Ask the LLM model to generate the answer to the given query. It is different from <code>qa</code> function, which just return the answer if the documents contain the information. This function will generate the answer from the LLM model.</p>"},{"location":"rag/#thml.rag.RAG.search","title":"<code>search(question: str = 'summary') -&gt; list[Document]</code>","text":"<p>Search information from the documents. This is actually perform <code>retriever.invoke</code> to retrieve information from <code>vectorstore</code>. Refer: https://python.langchain.com/docs/use_cases/question_answering/quickstart#retrieval-and-generation-retrieve.</p> <p>Parameters:</p> <ul> <li> <code>query</code>               (<code>str</code>)           \u2013            <p>The query to search for.</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>The number of documents to return.</p> </li> </ul> <p>Returns:     results (list[Document]): The documents that match the query.</p>"},{"location":"rag/#thml.rag.RAG.set_retriever","title":"<code>set_retriever(search_type: str = 'similarity', search_kwargs: dict = None)</code>","text":"<p>Define parameters for the retriever. See <code>vectorstore.as_retriever</code> for more information. Ref: https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore</p> <p>Parameters:</p> <ul> <li> <code>search_type</code>               (<code>Optional[str]</code>, default:                   <code>'similarity'</code> )           \u2013            <p>Defines the type of search that the Retriever should perform. Can be \"similarity\" (default), \"mmr\", or</p> </li> <li> <code>search_kwargs</code>               (<code>Optional[Dict]</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments to pass to the search function. Can include things like: k: Amount of documents to return (Default: 4) score_threshold: Minimum relevance threshold for similarity_score_threshold fetch_k: Amount of documents to pass to MMR algorithm (Default: 20) lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5) filter: Filter by document metadata</p> </li> </ul>"},{"location":"rag/#thml.rag.RAG.set_chain","title":"<code>set_chain(style: str = 'simple') -&gt; None</code>","text":"<p>Set the style of the RAG system. The style can be 'simple', 'multi_query', or 'fusion'.</p>"},{"location":"rag/#thml.rag.embedding_model","title":"<code>embedding_model(provider: str = 'huggingface', model_name: str = None, model_kwargs: dict = None) -&gt; object</code>","text":"<p>Define the embedding function to use. See the list of available models of Langchain</p> <p>Check the latest performance benchmarks for text embedding models at MTEB leaderboards hosted by Hugging Face. The fields to consider are:     - Score: the score we should focus on is \"average\" and \"retrieval average\". Both are highly correlated, so focusing on either works.     - Sequence length tells us how many tokens a model can consume and compress into a single embedding. Generally speaking, we wouldn't recommend stuffing more than a paragraph of heft into a single embedding - so models supporting up to 512 tokens are usually more than enough.     - Model size: the size of a model indicates how easy it will be to run. All models near the top of MTEB are reasonably sized. One of the largest is instructor-xl (requiring 4.96GB of memory), which we can easily run on consumer hardware.</p> Note <ul> <li>Embedding model may be referred to as SentenceTransformer in HF.</li> </ul> <p>Some HF's embedding models:     - mixedbread-ai/mxbai-embed-large-v1     - BAAI/bge-large-en-v1.5</p> <p>Parameters:</p> <ul> <li> <code>provider</code>               (<code>str</code>, default:                   <code>'huggingface'</code> )           \u2013            <p>The provider of the embeddings.</p> </li> <li> <code>model_name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the model to use for the reranker.</p> </li> <li> <code>model_kwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The parameters to use for the reranker.</p> </li> </ul>"},{"location":"rag/#thml.rag.llm_model","title":"<code>llm_model(service: str = 'web_opengpts', **kwargs: dict) -&gt; LLM</code>","text":"<p>Predefined LangChain's style LLM models to be used in the RAG system. Args:     service (str): The LLM model service. Available options: 'openai', 'web_openai', 'web_opengpts', 'web_phind', 'web_llama2', 'web_bing'     **kwargs: The model parameters, depend on the service. Returns:     LLM: The LangChain's LLM.</p>"},{"location":"rag/#thml.rag.load_document","title":"<code>load_document(doc_path: str = '', ext: str = None) -&gt; list[Document]</code>","text":"<p>Load documents from the given path, using langchain's [document_loaders]. Supported file types: .pdf, .docx, .txt, .md, .lnk (Windows' shortcuts).</p> <p>Parameters:</p> <ul> <li> <code>doc_path</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The path to the folder containing the documents.</p> </li> <li> <code>ext</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The file extension of the documents to be loaded, e.g., '.pdf'. Default, loads all files in the folder.</p> </li> </ul> <p>Returns:     list[Document]: A list of Document objects, containing the loaded documents.</p>"},{"location":"search_jobs/","title":"Search jobs","text":""},{"location":"search_jobs/#thml.search.jobs","title":"<code>thml.search.jobs</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed","title":"<code>Indeed(country: str = 'US')</code>","text":"<p>               Bases: <code>_JobBase</code></p>"},{"location":"search_jobs/#thml.search.jobs.Indeed.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.max_jobs","title":"<code>max_jobs = 100</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.jobs","title":"<code>jobs = []</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.country","title":"<code>country = country</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.base_url","title":"<code>base_url = 'https://www.indeed.com/stc'</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.Indeed.search_jobs","title":"<code>search_jobs(search_string: str = 'Molecular Dynamics Simulation', location: str = 'United States', filters: dict = None, detail: bool = False, max_jobs: int = 100)</code>","text":"<p>Search jobs on the website</p> <p>Parameters:</p> <ul> <li> <code>search_string</code>               (<code>str</code>, default:                   <code>'Molecular Dynamics Simulation'</code> )           \u2013            <p>a string to search for jobs</p> </li> <li> <code>location</code>               (<code>str</code>, default:                   <code>'United States'</code> )           \u2013            <p>location to search for jobs. A string in form \"city, state, zip\", or \"remote\"</p> </li> <li> <code>filters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>filters to apply. Available keys: \"distance\", \"job_type\", \"min_salary\", \"date_posted\"</p> </li> <li> <code>detail</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>get job details or not</p> </li> <li> <code>max_jobs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>maximum number of jobs to get</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>list[dict]: list of job items</p> </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.Indeed.get_job_single_page","title":"<code>get_job_single_page() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from search result on single page</p>"},{"location":"search_jobs/#thml.search.jobs.Indeed.get_job_multi_pages","title":"<code>get_job_multi_pages() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from multi pages</p>"},{"location":"search_jobs/#thml.search.jobs.Indeed.get_job_detail","title":"<code>get_job_detail() -&gt; list</code>  <code>async</code>","text":"<p>Get details of jobs from job lists</p>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired","title":"<code>SimplyHired(country: str = 'US')</code>","text":"<p>               Bases: <code>_JobBase</code></p>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.max_jobs","title":"<code>max_jobs = 100</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.jobs","title":"<code>jobs = []</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.base_url","title":"<code>base_url = 'https://www.simplyhired.com/'</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.jobperpage","title":"<code>jobperpage = 20</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.SimplyHired.search_jobs","title":"<code>search_jobs(search_string: str = 'Molecular Dynamics Simulation', location: str = 'United States', filters: dict = None, detail: bool = False, max_jobs: int = 100)</code>","text":"<p>Search jobs on the website</p> <p>Parameters:</p> <ul> <li> <code>search_string</code>               (<code>str</code>, default:                   <code>'Molecular Dynamics Simulation'</code> )           \u2013            <p>a string to search for jobs</p> </li> <li> <code>location</code>               (<code>str</code>, default:                   <code>'United States'</code> )           \u2013            <p>location to search for jobs. A string in form \"city, state, zip\", or \"remote\"</p> </li> <li> <code>filters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>filters to apply. Available keys: \"distance\", \"job_type\", \"min_salary\", \"date_posted\"</p> </li> <li> <code>detail</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>get job details or not</p> </li> <li> <code>max_jobs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>maximum number of jobs to get</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>list[dict]: list of job items</p> </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.get_job_single_page","title":"<code>get_job_single_page() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from search result on single page</p>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.get_job_multi_pages","title":"<code>get_job_multi_pages() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from multi pages</p>"},{"location":"search_jobs/#thml.search.jobs.SimplyHired.get_job_detail","title":"<code>get_job_detail() -&gt; list</code>  <code>async</code>","text":"<p>Get details of jobs from job lists</p>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn","title":"<code>LinkedIn(cookie_file: str = None)</code>","text":"<p>               Bases: <code>_JobBase</code></p>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn.browser_kwargs","title":"<code>browser_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.context_kwargs","title":"<code>context_kwargs = {}</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.page","title":"<code>page = None</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.max_jobs","title":"<code>max_jobs = 100</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.jobs","title":"<code>jobs = []</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.base_url","title":"<code>base_url = 'https://www.linkedin.com/jobs'</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.jobperpage","title":"<code>jobperpage = 25</code>  <code>instance-attribute</code>","text":""},{"location":"search_jobs/#thml.search.jobs.LinkedIn.search_jobs","title":"<code>search_jobs(search_string: str = 'Molecular Dynamics Simulation', location: str = 'United States', filters: dict = None, detail: bool = False, max_jobs: int = 100)</code>","text":"<p>Search jobs on the website</p> <p>Parameters:</p> <ul> <li> <code>search_string</code>               (<code>str</code>, default:                   <code>'Molecular Dynamics Simulation'</code> )           \u2013            <p>a string to search for jobs</p> </li> <li> <code>location</code>               (<code>str</code>, default:                   <code>'United States'</code> )           \u2013            <p>location to search for jobs. A string in form \"city, state, zip\", or \"remote\"</p> </li> <li> <code>filters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>filters to apply. Available keys: \"distance\", \"job_type\", \"min_salary\", \"date_posted\"</p> </li> <li> <code>detail</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>get job details or not</p> </li> <li> <code>max_jobs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>maximum number of jobs to get</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>list[dict]: list of job items</p> </li> </ul>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn.get_job_single_page","title":"<code>get_job_single_page() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from search result on single page</p>"},{"location":"search_jobs/#thml.search.jobs.LinkedIn.get_job_multi_pages","title":"<code>get_job_multi_pages() -&gt; list</code>  <code>async</code>","text":"<p>Get jobs from multi pages</p>"},{"location":"search_news/","title":"Search news","text":""},{"location":"search_news/#thml.search.news","title":"<code>thml.search.news</code>","text":""},{"location":"search_news/#thml.search.news.GooNews","title":"<code>GooNews(language='en', country='US', max_results=100, period=None, start_date=None, end_date=None, exclude_websites=None, proxy=None)</code>","text":"<p>               Bases: <code>GNews</code></p> <p>A class that allows you to search for news articles using Google News.</p> <p>Parameters:</p> <ul> <li> <code>language</code>               (<code>str</code>, default:                   <code>'en'</code> )           \u2013            <p>The language in which to return results. Defaults to en</p> </li> <li> <code>country</code>               (<code>str</code>, default:                   <code>'US'</code> )           \u2013            <p>The country code of the country you want to get headlines for. Defaults to US</p> </li> <li> <code>max_results</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The maximum number of results to return. The default is 100. Defaults to 100</p> </li> <li> <code>period</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The period of time from which you want the news</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Date after which results must have been published</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Date before which results must have been published</p> </li> <li> <code>exclude_websites</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>A list of strings that indicate websites to exclude from results</p> </li> <li> <code>proxy</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The proxy parameter is a dictionary with a single key-value pair. The key is the</p> </li> </ul>"},{"location":"search_news/#thml.search.news.GooNews.get_article","title":"<code>get_article(url)</code>","text":"<p>Download an article from the specified URL, parse it, and return an article object.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The URL of the article you wish to summarize.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>The article as defined by package <code>newpaper4k</code>, see here: https://newspaper4k.readthedocs.io/en/latest/user_guide/api_reference.html.</p> </li> <li>           \u2013            <p>This is different from the previous implementation which used <code>newspaper3k</code>.</p> </li> </ul>"},{"location":"search_news/#thml.search.news.GooNews.download_article_material","title":"<code>download_article_material(url, output_dir='./article_material')</code>","text":"<p>Download the article's text, images, and videos to the specified directory.</p> <p>Parameters:</p> <ul> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The URL of the article you wish to download.</p> </li> <li> <code>output_dir</code>               (<code>str</code>, default:                   <code>'./article_material'</code> )           \u2013            <p>The directory to save the article's material to. Defaults to \"./article_material\".</p> </li> </ul>"},{"location":"search_news/#thml.search.news.news_by_topic","title":"<code>news_by_topic(topic: str)</code>","text":"<p>Generate news video by topic.</p> <p>Tasks: 1. Get google news by topic 2. Download article material (text, images, videos). May be only text available. 3. Search related videos (youtube, google,...) and download them. 4. Generate video</p>"},{"location":"search_news/#thml.search.news.news_generator","title":"<code>news_generator()</code>","text":"<p>Generate news videos for a list of topics.</p> <p>Tasks: Run <code>news_by_topic</code> for each topic in a given list.</p>"},{"location":"util/","title":"util","text":""},{"location":"util/#thml.util","title":"<code>thml.util</code>","text":""},{"location":"util/#thml.util.html_diffs","title":"<code>html_diffs(a: str, b: str) -&gt; str</code>","text":"<p>Return the side-by-side HTML of the differences between text_a and text_b.</p> <p>Parameters:</p> <ul> <li> <code>a</code>               (<code>str</code>)           \u2013            <p>The first string.</p> </li> <li> <code>b</code>               (<code>str</code>)           \u2013            <p>The second string.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>html</code> (              <code>str</code> )          \u2013            <p>The side-by-side HTML of the differences between text_a and text_b.</p> </li> </ul>"}]}